# BROKER MQ KAFKA

IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII

IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII

## ğŸ“Œ Apache Kafka

`Kafka` is open source and developing by `Apache`. But `Confluent`'s `Kafka` is a downstream (fork) of `Apache` `Kafka` which has additional features.

## ğŸ“Œ Zookeeper

`Kafka` requires `Zookeeper` for the following reasons:

- store leader and follower information
- which nodes are available in cluster
- list of `topic`s
- which `topic`s are storing in which nodes and `partition` lists
- Quotas - how much data is each client allowed to read and write
- `ACL`s - who is allowed to read and write to which `topic` (old high level consumer) - Which consumer groups exist, who are their members and what is the latest offset each group got from each `partition`.

## ğŸ“Œ Command line app

In the `/kafka-server/bin/` directory there are command line scripts which can easily allow developer to call the APIs of `Kafka` server. Examples:

```sh
## ğŸ“Œ first we start the Kafka server
/bin/kafka-server-start.sh "config/server.properties"

/bin/kafka-topics.sh --Zookeeper "localhost:2181" --delete --topic "DEMO"

/bin/kafka-console-producer.sh --broker-list "localhost:9092" --topic "sampleTopic"

/bin/kafka-topics.sh --create --Zookeeper "localhost:2181" --replication-factor "1" --partitions "1" --topic "sampleTopic"
```

As it can be seen on above example, operations like creating or removing `topic`s requires `Zookeeper`'s socket information. Because this requests are not sending to `Kafka`, they sending directly to `Zookeeper`. But the commands like consuming or publishing messages are sending directly to `Kafka` server.

If we read the script file: (`source-id: 372`), we will see that it calls this class: (`source-id: 373`). As it can be seen inside the `Scala` `org.I0Itec.zkclient.ZkClient` class, it sends request to `Zookeeper` via `Zookeeper` client.

`Kafka` can send requests to `Zookeeper` to create or delete `topic`s. Those APIs are considered as admin operations. `(source-id: 375) title: "Running from JAR", paragraph: 2 + (source-id: 376)`

## ğŸ“Œ Consumer reads via partition and topic

Consumer'lar sadece spesifik `partition`'lar ve `topic`'ten mesaj Ã§ekebilir:

```java
// example using spring-boot
@KafkaListener(topicPartitions = @TopicPartition(topic = "topicName", partitions = { "0", "1" }))
```

Fakat `partition` bilgisi vermezse sadece `topic` bilgisi verirse, o zaman `Kafka` server, o client'a kendi bir `partition` atar. `Kafka` yÃ¼kÃ¼ tÃ¼m client'lara dengeli daÄŸÄ±tÄ±r.

## ğŸ“Œ CustomPartitioner class

`Java`'da producer tarafta `CustomPartitioner` class'Ä± ile, giden mesajÄ±n hangi `parition`'a gideceÄŸini belirleyebiliriz. MesajÄ±n iÃ§eriÄŸinden bu ID'yi elde edecek metodu yazmamÄ±z ÅŸart.

`Partition` ID, interger olmak zorunda.

Fakat eÄŸer `CustomPartitioner` kullanmazsak client ÅŸu ÅŸekilde mesajÄ± yollar:

- EÄŸer mesaj key'imiz `null` ise, `topic` yaratÄ±lÄ±rken oluÅŸturulmuÅŸ tÃ¼m `partition`'lara `round-robin` ile mesajlarÄ± yÃ¼kler.

- EÄŸer mesaj key'imiz `null` deÄŸil ise, onun `hash`'ini alÄ±r ve `modulo` iÅŸlemine tabi tutar.

## ğŸ“Œ ACK of producer

producer tarafÄ±nda ayarlanan bir parametredir. bu parametre ÅŸu deÄŸerleri alabilir:

- `0` (`at most once`)

  client isteÄŸi `Kafka`'ya atar. isteÄŸin bir yere ulaÅŸÄ±p ulaÅŸmadÄ±ÄŸÄ± ile hiÃ§ ilgilenmez.

- `1` (`at least once`)

  client sadece `kafka` leader'Ä±n isteÄŸi aldÄ±ÄŸÄ±ndan emin olana kadar bekler.

- `-1` (`all`)

  o `partition` iÃ§in tÃ¼m diÄŸer in-sync replica `kafka` node'larÄ±nÄ±n bu record'u aldÄ±ÄŸÄ±ndan emin olana kadar bekler.

`kaynak: (source-id: 387) "Apache Kafka ack-value" baÅŸlÄ±ÄŸÄ±.`

## ğŸ“Œ ACK of consumer

`Committing an offset` means that consumer got the message. That means a spesifik record is "`consumed`".

Consumer `kafka`'ya mesajÄ± iÅŸlediÄŸine dair mesaj atabilir. bu mesaj standarttÄ±r ve farklÄ± tipi yoktur. fakat client kendi istediÄŸi zaman `kafka` node'una ne zaman dÃ¶neceÄŸini kendi belirler.

### ğŸ“ŒğŸ“Œ At Least Once

`spring`'de `ENABLE_AUTO_COMMIT_CONFIG` ayarÄ±nÄ± false yaparsak, `Java` tarafÄ±nda manuel onaylama yapmamÄ±z gerekir:

```java
@KafkaListener(topics = "your-topic")
public void consumeMessage(String message, Acknowledgment ack) {
    
    // do some business here.
    
    // at the end of the business:
    ack.acknowledge(); 
    // this block also should be also inside "exception catch" blocks. Otherwise kafka will not receive "commited" signal, therefor Kafka will send same message again and again.
}
```

### ğŸ“ŒğŸ“Œ At Most Once

`spring`'de `ENABLE_AUTO_COMMIT_CONFIG` ayarÄ±nÄ± true yaparsak, mesaj iÅŸlendikten sonra `Spring` otomatik olarak `Kafka`'ya `commited` sinyalini yollar. 

Burada `Spring`, `consumeMessage` metodumuz hata fÄ±rlatsa dahi, `commited` mesajÄ±nÄ± `Kafka`'ya yollar. Ã§Ã¼nkÃ¼ aksi durumda `kafka` sÃ¼rekli aynÄ± mesajÄ± atacak.

## ğŸ“Œ record (âŸ· message)

- The each message which send by client to `Kafka` server.
- Each record is key-value.
- `Kafka` guarantees the consumption of messages in order (The order is based on `partition` only - not `topic`!).

## ğŸ“Œ partition

It is the same thing with the `partition` term that we use on other `NoSQL`. In many `NoSQL`, we distribute the data depending on the `partition` ID.

In most `NoSQL`, as default the `partition` ID is the `hash` of the data (on `Kafka` the "`record`"). On `kafka` we can implement `CustomPartitioner` class to choose the `partition` of each record.

Some rules:

- Each `topic` must store at least in one `partition`.
- 1 `partition` must have only 1 `topic`.

`Partition` ID and `topic` are both unique together. In other words, `user-created-topic` has `partition` ID `1`, but `warnings-topic` may also have `partition` ID `1`. Both these `partition`s are completely independent.

`Topic`s are logical concepts, `partition`s are physically divided portions.

`Partition`s are like queues. We can consume a record from `partition` using `partition` id and `topic` name. `(source-id: 379) title: "KafkaConsumer", 3rd code example.`

To understand clearly we can give real examples:

- `Topic`: `completed-payment-transactions`, `Partition`: `Store ID`
- `Topic`: `order-started`, `Partition`: `Store ID`

Each store should have is equivalent `partition` ID. Examples:

- store id: 100 ---> `partition` id: 1
- store id: 200 ---> `partition` id: 2
- store id: 400 ---> `partition` id: 4

The `partition` ID's on `Kafka` start from zero. So we should find a mathematical operation to find the equivalent `partition` ID from store ID.

`(source-id: 379) title: "KafkaConsumer", 3rd code example is written on Python.` But the same object exist also on `Java`. `TopicPartition` instance is creating using `topic` name and the `partition` ID. `(source-id: 377)`:

```java
TopicPartition(String topic, int partition)
```

## ğŸ“Œ consumer group

`Kafka` does not store which consumer has read which data. Therefore each client should remember it's own `offset` (`topic` `offset` and `partition` ID) he read last. But `kafka` has another optional feature: `Kafka` stores the last read record (`offset`) for each `consumer group`. Therefore, as a client, if you don't have a specific reason, you don't have to manage `offset`s.

- birden fazla consumer, 1 adet `consumer group` altÄ±nda olabilir.
- Kafka 1 `topic`'teki `partition`'larÄ± `consumer group`'taki her client'a dengeli daÄŸÄ±tÄ±r. Ã¶rnekler:
  - 4 `partition` varsa, 2 client varsa, ilk 2 `partition`'Ä± ilk client'a, 2inci ve 3Ã¼ncÃ¼ `partition`'larÄ± 2inci client'a daÄŸÄ±tÄ±r.
  - 4 `partition` varsa, 5 client varsa, 1 client boÅŸta bekler.
- 1 client 1'den fazla `consumer group`'un mensubu olabilir.
- 1 `topic`'ten birden fazla `consumer group` birbirlerinden baÄŸÄ±msÄ±z ÅŸekilde, baÄŸÄ±msÄ±z `offset`'lerle, aynÄ± `topic`'ten mesaj okuyabilir.
- kafka `consumer group`' bilgisini server'da tutarken ÅŸu ÅŸekilde tutar:

  - `consumer group`:user-service, `topic`:X, `partition`:1, last-read-offset:99
  - `consumer group`:payment-service, `topic`:X, `partition`:2, last-read-offset:88
  - `consumer group`:order-service, `topic`:Y, `partition`:1, last-read-offset:77

## ğŸ“Œ offset

`offset` meaning in IT: the distance of an element (point) to the reference (generally: position zero) element (point).

Each record has an `offset` in a specific `partition`. This value never changes.

Yeni `partition`'lar, 0 offset'i ile yaratÄ±lÄ±r. Fakat yeni mesajlar geldikten sonra, o `partition`'daki eski tÃ¼m mesajlar silinirse bile, o `partition`'un first offset'i 0 olmaz. silinen mesajlarÄ±n sayÄ±sÄ± kadar olur. 30 mesaj silindiyse, first offset 30 olur.

## ğŸ“Œ replication factor

her `partition`'un kaÃ§ adet broker'da klonlanacaÄŸÄ±nÄ± belirtebiliyoruz.

client mesajÄ± sadece 1 node'a yollar, mesajÄ± alan node, mesajÄ± leader node'a, leader ise diÄŸer follower node'lara yollar.

her `partition` iÃ§in __leader__ ve __follower (âŸ· in-sync replicas âŸ· ISR)__ broker'lar vardÄ±r. follower'lar (ilgili `partition` iÃ§in) sadece data'nÄ±n klonlanmasÄ±nda gÃ¶revlidirler. aynÄ± zamanda leader Ã§Ã¶kerse follower'lardan biri leader olarak devreye girmektedir. `kaynak: (source-id: 381) "Replication: Kafka Partition Leaders, Followers, and ISRs." baÅŸlÄ±ÄŸÄ±. + DiÄŸer kaynak: (source-id: 382) "What is Replication?" baÅŸlÄ±ÄŸÄ± "Leader for a partition:" ile baÅŸlayan 7inci paragraf.`

## ğŸ“Œ Add new partitions to existing topics

Bu Ã§ok kolay ÅŸekilde yapÄ±labiliyor:

```sh
bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --partitions 9 
```

Fakat `Kafka` arka planda eski data'larÄ± ilgili `partition`'lara taÅŸÄ±maz.

`./bin/kafka-reassign-partitions.sh` script'i ile var olan `partition`'larÄ± farklÄ± broker'lara taÅŸÄ±ma tarzÄ± iÅŸlemler yapÄ±labilmektedir.

## ğŸ“Œ Kafdrop

`Kafka` iÃ§in aÃ§Ä±k kaynaklÄ± `Web UI` monitoring uygulamasÄ±.

Sadece temel Ã¶zellikleri iÃ§eriyor.

## ğŸ“Œ MirrorMaker

`Kafka` default'ta (tek baÅŸÄ±na) multi data center hizmeti mevcut deÄŸil. Bunun iÃ§in `Kafka` sunucusundan baÄŸÄ±msÄ±z yazÄ±lÄ±mlardan yararlanmak gerekiyor. Bunlardan biri de `MirrorMaker`.

`MirrorMaker` `Kafka`'lardan baÄŸÄ±msÄ±z olarak ayaÄŸa kaldÄ±rÄ±lÄ±yor. AyaÄŸa kaldÄ±rÄ±rken tÃ¼m data center'lardaki her `Kafka`'nÄ±n IP ve port bilgileri config ile `MirrorMaker`'e veriliyor. AynÄ± zamanda `MirrorMaker`'a, hangi `Kafka`'lardaki `topic`'lerin hangi `Kafka`'lara sync olacaÄŸÄ± gibi bilgiler veriliyor. Bu bilgiler doÄŸrultusunda `MirrorMaker` consumer ve producer olarak davranÄ±p, data center'lar arasÄ±nda bu `Kafka`'larÄ± sync etmeye Ã§alÄ±ÅŸÄ±yor.

`Confluent` firmasÄ±nÄ±n geliÅŸtirdiÄŸi `Replicator`, `MirrorMaker` alternatifidir.

## ğŸ“Œ Kafka connect

`Kafka`'dan baÄŸÄ±msÄ±z Ã§alÄ±ÅŸan bir instance'dÄ±r. 2 farklÄ± connector'Ã¼ vardÄ±r:

- __Source connector__

  baÄŸÄ±msÄ±z sistemlerden (Ã¶rneÄŸin DB'den), data okur ve bunlarÄ± `Kafka`'da `topic`'lere kaydeder.

- __Sink connector__

  `Kafka` `topic`'lerden okuduÄŸu data'larÄ± baÄŸÄ±msÄ±z bir sisteme (Ã¶rnek: BD'ye) yazar.

## ğŸ“Œ Kafka Streams

Hem consumer hem de producer iÃ§in API'dir. Bu API, `Java`'daki stream altyapÄ±sÄ±na uygun metotlar sunar.
