# DB CASSANDRA

IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII

IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII

## ğŸ“Œ Cassandra

Cassandra yazÄ±lÄ±mÄ±, Apache foundation tarafÄ±ndan Ã¼cretsiz ve aÃ§Ä±k kaynaklÄ± olarak sunulmaktadÄ±r. __DataStax__ firmasÄ± Apache'nin repo'sunu base alarak; "__DataStax Enterprise (âŸ· DSE)__" isminde Ã¼rÃ¼n sunmaktadÄ±r. __DataStax__ hem Ã¼cretli hem Ã¼cretsiz versiyon iÃ§ermektedir. __DataStax__ sadece ek Ã¶zellikler sunar ve teknik support verir.

Cassandra Java'da yazÄ±ldÄ±ÄŸÄ± iÃ§in runtime'da `JVM`'e ihtiyaÃ§ duyar.

- cluster iÃ§indeki node'lar arasÄ±nda master-slave iliÅŸkisi yoktur. onun yerine __ring design__ sÃ¶z konusudur. Bu durumda `SPOF` olmaz. her node isteklere cevap verebilecek tasarÄ±mdadÄ±r.
- Cassandra cluster yapÄ±sÄ± iÃ§erisinde "ring design" ile haberleÅŸtiÄŸi iÃ§in "cluster" terimi yerine "ring" terimi de kullanÄ±lmaktadÄ±r.
- multi data center support'u default olarak vardÄ±r. yani; WAN Ã¼zerinden dahi, aynÄ± cluster'a baÄŸlÄ± Cassandra node'larÄ± birbiri arasÄ±nda paylaÅŸÄ±m yapabilir. bunun iÃ§in ek bir servise ihtiyaÃ§ yoktur. bu Ã¶zellik Cassandra sunucularÄ±nda default gelir.
- client cluster iÃ§erisindeki herhangi bir node'a istek yapabilir. Client'Ä±n attÄ±ÄŸÄ± isteÄŸi ilk alan node'a __coordinator (âŸ· koordinatÃ¶r)__ ismi verilir. eÄŸer bu istek query isteÄŸi ise; return data'sÄ± koordinatÃ¶rde yok ise; koordinatÃ¶r arka planda bu bilgiyi diÄŸer node'lara sorarak dÃ¶nÃ¼ÅŸ yapar. eÄŸer istek kaydetme isteÄŸi ise; koordinatÃ¶r fiziksel olarak o data'yÄ± kaydedecek olan node'lara isteÄŸi yollar.
- __Ayarlanabilir Veri TutarlÄ±lÄ±ÄŸÄ± Seviyesi (âŸ· Consistency Levels)__: Veri tutarlÄ±lÄ±ÄŸÄ± seviyesi, okuma isteÄŸinin kaÃ§ sunucudan veri alarak cevaplanacaÄŸÄ±nÄ± ve yazma isteÄŸinin kaÃ§ sunucuya veri yazÄ±ldÄ±ktan sonra istemciye "tamamlandÄ±" olarak dÃ¶nÃ¼leceÄŸini belirler. Ã–rneÄŸin kopya sayÄ±sÄ± 10 olan bir kÃ¼meye gelen yazma isteÄŸinin tutarlÄ±lÄ±k seviyesi 3 ise, gelen veri 3 sunucuya yazÄ±ldÄ±ktan sonra (kalan 7 sunucuya kopyalama devam ederken) istemciye "tamamlandÄ±" olarak dÃ¶nÃ¼lecektir. Cassandra'da veri tutarlÄ±lÄ±ÄŸÄ± seviyesi kÃ¼me ve sorgu bazÄ±nda ayarlanabilir.
- eÄŸer client data manipÃ¼lasyonu yapacak istek yollarsa sÄ±rasÄ± ile ÅŸunlar iÅŸletilir:
  - Cassandra node'u gelen isteÄŸi fiziksel bellekte olan "commit log" iÃ§ine kaydeder
  - client'a "committed" mesajÄ±nÄ± dÃ¶ner
  - data'yÄ± __Memtable__ denilen hÄ±zlÄ± hafÄ±zadaki bÃ¶lgeye taÅŸÄ±r
  - data'yÄ± olmasÄ± gereken yere taÅŸÄ±r. yani sorting'e gÃ¶re olmasÄ± gerektiÄŸi yere taÅŸÄ±r. En son taÅŸÄ±nan bÃ¶lgeye __SSTable__ adÄ± verilir.
- Okuma iÅŸlemlerinde ise Ã¶nce node'daki Memtable'a bakÄ±lÄ±r, eÄŸer yok ise SSTable'a bakÄ±lÄ±r.

## ğŸ“Œ insert into vs update

"insert into" iÅŸlemi if not exist Ã¶zelliÄŸi iÃ§ermiyor ve varsa aynÄ± ID'li data'yÄ± override ediyor. Bu durumda "insert into" ile "update" aynÄ± gÃ¶revi gÃ¶rÃ¼yor. fakat ikisi arasÄ±nda ufak farklÄ±lÄ±klar var. buradaki gibi: https://stackoverflow.com/questions/16532227/difference-between-update-and-insert-in-cassandra answer of billbaird at "May 16 '14 at 23:04". Stackoverflow'da billbaird'in bahsettiÄŸi konunun bug olmadÄ±ÄŸÄ±nÄ± gÃ¶steren kaynak: https://issues.apache.org/jira/browse/CASSANDRA-11805

Cassandra "lock" mekanizmasÄ±nÄ± desteklemez. (oysa bu durum distributed sistemlerde bu Ã¶nemli bir gereksinimdir.)

Client Cassandra'ya bir kayÄ±t isteÄŸi attÄ±ÄŸÄ±nda Cassandra hÄ±zlÄ± dÃ¶nÃ¼ÅŸ yapabilmek iÃ§in Ã¶nce commit log'a yazar ve client'a dÃ¶nÃ¼ÅŸ yapar (bu konu baÅŸka baÅŸlÄ±kta detaylÄ± anlatÄ±lÄ±yor). iÅŸte performance avantajÄ± burada. commit log'daki data daha sonra gerekli yere fiziksel olarak taÅŸÄ±nÄ±r. Burada ince bir nokta var: Commit log'a sadece kayÄ±t atÄ±labiliyor. Ä°ÅŸte bu sebeple Cassandra'da update ve insert iÅŸlemi aynÄ± ÅŸeye denk geliyor. AynÄ± sebepten, Cassandra "lock" mekanizmasÄ± da destekleyemiyor. Ã‡Ã¼nkÃ¼ commit log'a herhangi bir client bir bilgi atabilir. commit log hiÃ§bir kontrol iÃ§ermiyor. Bu sebeple Ã§ok hÄ±zlÄ±. EÄŸer kontroller gelirse performance kaybÄ± olur. Bu durumda farklÄ± DB server'larÄ± kullanmamÄ±z daha avantajlÄ± olur. Cassandra zaten generic purpose bir DB deÄŸil. BazÄ± durumlarda kullanmak gerekiyor.

Cassandra ek olarak istisna durumlar iÃ§in `if not exist` Ã¶zelliÄŸi de sunuyor. Bu `lightweight transactions (âŸ· LWT)` isimli bir Ã¶zellik ile destekliyor fakat bu pek Ã¶nerilmiyor.

## ğŸ“Œ SSTables

Cassandra'nÄ±n yazma hÄ±zÄ±nÄ±n yÃ¼ksek olmasÄ±nÄ±n sebeplerden biridir. SSTables'lar birer fiziksel dosyadÄ±r. her dosya immutable'dÄ±r. yani; dosyanÄ±n iÃ§eriÄŸini update edilmez. Memtable direk olarak SSTable olarak fiziksel olarak saklanÄ±r.

Bu sebeple Cassandra'da silme veya update iÅŸlemi gerÃ§ekten fiziksel kaydÄ± silmez/deÄŸiÅŸtirmez. Silme veya update iÅŸlemi de yeni bir row'dur(kayÄ±ttÄ±r). update edilen kayÄ±t (row) okunacaÄŸÄ± zaman her zaman en sondaki kayÄ±t okunur.

SSTables'larÄ±n sayÄ±sÄ± arttÄ±kÃ§a, Cassandra arka planda iÅŸe yaramayan satÄ±rlarÄ± (kayÄ±tlarÄ±) ignore eder/siler ve hala kullanÄ±lmasÄ± gereken kayÄ±tlarÄ± yeni SSTable'lara merge eder. Bu iÅŸleme "compaction" denir.

Silme iÅŸleminde, silmek iÃ§in atÄ±lan event'e __tombstone (âŸ· mezar taÅŸÄ±)__ adÄ± verilir. Tombstone ile iÅŸaretlenen kayÄ±tlar compaction ile sinir ve beraberinde tombstone'larda silinir.

## ğŸ“Œ Cassandra vs MongoDB

Burada birÃ§ok detay farklÄ±lÄ±k Ã§Ä±kabilir. Fakat tÃ¼m sebepler aÅŸaÄŸÄ±daki temel farklÄ±lÄ±klarÄ±n sonucunda ortaya Ã§Ä±kmaktadÄ±r:

### ğŸ“ŒğŸ“Œ cluster yapÄ±sÄ±

MongoDB'de auto-election Ã¶zelliÄŸi var. Tek bir MongoDB instance'Ä± isteklere cevap dÃ¶ner (master node). DiÄŸer node'lar (slave node'lar) arka planda gÃ¼ncel data'lara sync olur. EÄŸer master node Ã§Ã¶kerse, auto-election devreye girer ve diÄŸer node'lerdan biri master olur. Burada ortalama 12 saniye zaman kaybÄ± yaÅŸanacaktÄ±r fakat isteÄŸe baÄŸlÄ± olarak bu arada read-query'lerine cevap verilebilir. BazÄ± kaynaklarda bu sÃ¼renin ortalama 1 dakika olduÄŸu yazar. Oysa Cassandra'da master-slave iliÅŸkisi yoktur. her node eÅŸit miktarda data'yÄ± fiziksel olarak barÄ±ndÄ±rÄ±r (tabi bazÄ± data'lar yine bu node'lar arasÄ±nda yedekli ÅŸekilde barÄ±ndÄ±rÄ±lÄ±r). gelen isteÄŸe cevap verecek data yok ise; diÄŸer node'a istek yÃ¶nlendirilir. EÄŸer isteÄŸi alan node Ã§Ã¶kerse, diÄŸer node'lar isteÄŸe cevap zaten veriyor durumdadÄ±r. Auto-election gibi bir sÃ¼rece ihtiyaÃ§ yok.

MongoDB'de slave node'lar (eÄŸer istenirse/ayarlanÄ±rsa) read isteklerine cevap verebildiÄŸi iÃ§in read'ler aÃ§Ä±sÄ±ndan Cassandra ve MongoDB aynÄ± performansÄ± sergiliyor diyebiliriz. Fakat iÅŸ yazmaya geldiÄŸinde Cassandra Ã§ok daha Ã¼stÃ¼n performance sergiliyor. Ã‡Ã¼nkÃ¼ Mongo'da sadece master node yazma iÅŸlerini hallederken, Cassandra'da tÃ¼m node'lar bu gÃ¶revi tamamlayabilme yeteneÄŸine sahip. yani; Cassandra'da hangi node ilk olarak isteÄŸi karÅŸÄ±ladÄ±ysa, o node kaydetme iÅŸini Ã¼stleniyor.

### ğŸ“ŒğŸ“Œ join queries

Cassandra'nÄ±n yapÄ±sÄ± gereÄŸi SQL dilindeki join gibi Ã¶zellikler Ã§alÄ±ÅŸmamaktadÄ±r. AynÄ± zamanda "where" ile sorgu her sÃ¼tun iÃ§in yapÄ±lamamaktadÄ±r. Bu sebeple sorgularda kÄ±sÄ±tlamalar yaÅŸanmaktadÄ±r. Benzer sebeplerden her sÃ¼tuna index'te atÄ±lamamaktadÄ±r. (AslÄ±nda bu paragrafta belirtilenler Cassandra ile de yapÄ±labilir, fakat eÄŸer yapÄ±lÄ±rsa Cassandra kullanmanÄ±n hiÃ§bir manasÄ± kalmaz.). Oysa `MongoDB` zaten nested dÃ¶kÃ¼manlarla Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in onlar iÃ§erisinde geliÅŸmiÅŸ sorgular yapabilmemizi destekliyor.

### ğŸ“ŒğŸ“Œ scheme

`Cassandra`'da tablo yapÄ±sÄ± (ÅŸemasÄ±) Ã¶nceden belli/sabit iken, `MongoDB`'de bÃ¶yle bir zorunluluk yoktur. `MongoDB`'de ÅŸama opsiyonel'dir.

Hatta `MongoDB`'de bir field farklÄ± kayÄ±tlarda farklÄ± tipte (int, string...) olabilir.

### ğŸ“ŒğŸ“Œ nested objects

`MongoDB`'de, iÃ§ iÃ§e (nested) JSON objeler tutulabilir. Oysa Cassandra'da bÃ¶yle bir imkan yoktur. Cassandra'da iÃ§ iÃ§e bilgi yerine, aynÄ± tabloda farklÄ± sÃ¼tunlara yazmak gerekiyor. Ã–rneÄŸin:

`MongoDB`:

```json
"order" : {
  "order_id": "99",
  "order_date" : "01 01 2002",
  "user" : {
    "user_id": "123",
    "user_name": "Ahmet",
    "user_surname": "AnySurName"
  }
}
```

Cassandra:

```json
"order" : {
  "order_id": "99",
  "order_date" : "01 01 2002",
  "user_id": "123",
  "user_name": "Ahmet",
  "user_surname": "AnySurName"
}
```

Cassandra'da daha sonradan "collection" yapÄ±larÄ± geldi.

collection'lar bir field'Ä±n iÃ§inde map, list tutabilmemizi saÄŸlÄ±yor. fakat gerÃ§ek bir sÃ¼tun gibi Ã§alÄ±ÅŸÄ±lmasÄ±nÄ± saÄŸlamÄ±yor. limitasyonlar var. Frozen olan collection'larda, frozen olan tÃ¼m collection update edilirken, tÃ¼m bilgiler gÃ¼ncelleniyor. frozen data'lar binary data olarak saklanÄ±yor. Frozen olan collection'larda limitasyonlar Ã§ok daha fazla.

`MongoDB` terminolojisinde tek dÃ¶kÃ¼man iÃ§inde olan diÄŸer (nested / iÃ§ iÃ§e) dÃ¶kÃ¼manlara __embedded document__ adÄ± verilir. ve Embedded dÃ¶kÃ¼manlar native olarak `MongoDB` tarafÄ±ndan desteklenir. Embedded kullanmak iÅŸlemleri hÄ±zlandÄ±rÄ±r fakat data duplication'Ä± arttÄ±rÄ±r. Onun yerine reference (`MongoDB` terminolojisinde `DEBRef`) kullanÄ±labilir fakat o zaman query'ler yavaÅŸlar fakat duplication azalÄ±r. AynÄ± durum Cassandra iÃ§in de vardÄ±r. Fakat Cassandra nested veya referanslarÄ± native olarak desteklemez. Onun yerine Cassandra'yÄ± kullanan yazÄ±lÄ±mÄ±n bunu manuel yapmasÄ± gerekir.

## ğŸ“Œ Cassandra veri yapÄ±sÄ±

- __Keyspace__, `RDBMS`'deki ÅŸemaya veya DB'ye denk gelmektedir. en Ã¼st seviyeli DB kavramÄ±dÄ±r.
- __column family__, `RDBMS`'deki tabloya denk gelmektedir. Java'da ÅŸu objede tutulur:

  ```java
  Map<PartitionKey, SortedMap<ColumnKey, ColumnValue>> // (daha ileride ne olduklarÄ± anlatÄ±lÄ±yor)
  ```

  Her satÄ±r aslÄ±nda Partition Key ile tutulur. Her satÄ±r'da birden fazla column-key ve her column-key'e denk gelen value vardÄ±r.

  AslÄ±nda bakÄ±ldÄ±ÄŸÄ±nda `RDBMS`'de de aynÄ± ÅŸekilde data'yÄ± temsil edebiliriz: her satÄ±rÄ±n mutlaka bir ID'si olsun. her satÄ±rda, bir sÃ¼tuna karÅŸÄ±lÄ±k gelen bir deÄŸer vardÄ±r. (bu konu baÅŸka baÅŸlÄ±kta anlatÄ±lÄ±yor: "column family")

- __column key (âŸ· column name)__ `RDBMS`'deki sÃ¼tun ismine denk gelmektedir. her column key'e karÅŸÄ±lÄ±k gelen bir de __column value__ vardÄ±r.

- __Cassandra Query Language (âŸ· CQL)__ dili ile query atÄ±lÄ±r. SQL'e Ã§ok benzerdir. __Cassandra query language shell (âŸ· cqlsh)__ komut satÄ±rÄ± uygulamasÄ± ile bu query'ler atÄ±labilir. CQL'de hala 'table' gibi terimler kullanÄ±lÄ±yor. cqlsh, shell/bat script'idir fakat kendi iÃ§erisinde Python'u Ã§aÄŸÄ±rÄ±r.

## ğŸ“Œ Keyspace

Keyspace yaratan bir metot yazalÄ±m:

```java
public void createKeyspace(String keyspaceName) {

  String queryStr = "CREATE KEYSPACE IF NOT EXISTS " + keyspaceName +
                    "WITH replication = {'class':'SimpleStrategy','replication_factor':'2'};";

  session.execute(queryStr);
}
```

Dikkat edilirse her keyspace iÃ§in ayrÄ± ayrÄ± replica sayÄ±sÄ±nÄ± verebiliyoruz. EÄŸer replica vermezsek; 1 satÄ±r 1 node'da yer alÄ±r. Fakat o tablonun her satÄ±rÄ± aynÄ± node'da olmak zorunda deÄŸildir. Cassandra node'larÄ± her satÄ±rÄ± kendi arasÄ±nda paylaÅŸmaktadÄ±r. EÄŸer tablonun replica sayÄ±sÄ±nÄ± 2 yaparsak, her satÄ±r mutlaka 2 node'da olacaktÄ±r. (Burada "satÄ±r" yerin "partition key" demek daha doÄŸru olacaktÄ±r)

YukarÄ±dakinden daha farklÄ± stratejiler mevcut. Ã–rneÄŸin; "NetworkTopologyStrategy". (baÅŸka baÅŸlÄ±kta anlatÄ±lÄ±yor)

## ğŸ“Œ Check all keyspace via CQL

```java
ResultSet result = session.execute("SELECT * FROM system_schema.keyspaces;");

List<String> allKeyspaceList = result.all();
```

## ğŸ“Œ DB design patterns (which are mostly used on Cassandra)

### ğŸ“ŒğŸ“Œ one table per query pattern (âŸ· one-table-per-query pattern) vs query-first approach (âŸ· query-driven modelling)

AralarÄ±nda farklÄ±lÄ±klar var. Cassandra kullanÄ±rken Ã§oÄŸu zaman "one table per query pattern" yaparÄ±z fakat bu en optimum Ã§Ã¶zÃ¼mÃ¼ bulduÄŸumuz anlamÄ±na gelmez. en optimum Ã§Ã¶zÃ¼m 'query first approach'tur.

table per query pattern'i aÃ§Ä±klamak gerekirse: selectByTitle sorgumuz iÃ§in Ã¶zel bir tablomuz olsun: "booksByTitle". Durum bÃ¶yle olunca duplicate data'larÄ±mÄ±z tÃ¼m DB'de Ã§oÄŸalÄ±yor. fakat bu durum Cassandra'da okumada performans artÄ±ÅŸÄ± saÄŸlanÄ±yor. ArtÄ±k her yeni kayÄ±t iÅŸlemlerimizde bir 'book' ve 'booksByTitle'a ekleme yapmamÄ±z gerekecek.

query first approach; ise farklÄ±dÄ±r. buradaki fark; query first approach'ta bazen Ã§ok benzer query'leri aynÄ± tabloya atabilmemizdir. Sadece where koÅŸulu deÄŸiÅŸebilir... Ã§Ã¼nkÃ¼  bazÄ± durumlarda Ã¶nceliÄŸi dÃ¼ÅŸÃ¼k sorgular iÃ§in tekrarlÄ± data olmamasÄ±nÄ± hÄ±za tercih edebiliriz. BÃ¶yle durumlarda primary key birden fazla sÃ¼tuna olur.

Ek not: Cassandra'da tablo isimlendirmeleri ÅŸu ÅŸekilde olmalÄ±: user_by_name, users_by_id, reservations_by_user_id..

### ğŸ“ŒğŸ“Œ time series pattern

activity log olarak dÃ¼ÅŸÃ¼nebileceÄŸimiz tablolardÄ±r. "money_transaction" tablosu buna en temel Ã¶rneklerden biridir. blockchain mantÄ±ÄŸÄ± gibi silinen data olmamasÄ± gerekir.

### ğŸ“ŒğŸ“Œ wide partition pattern (âŸ· wide row pattern)

iliÅŸkili data'larÄ±n aynÄ± partition'da tutulup, bu ÅŸekilde tek bir sorgu ile hÄ±zlÄ± Ã§aÄŸrÄ±lmasÄ± pattern'idir.

## ğŸ“Œ Chebotko notation

Artem Chebotko'nun geliÅŸtirdiÄŸi Cassandra iÃ§in tablo gÃ¶steriminde kullanÄ±lan notasyondur. temelde ÅŸu ÅŸekildedir:

```text
Q1 (Find User by name)
        â†“
*************************           *************************
*     users_by_name     *           * users_by_school_name  *
*************************           *************************
* user_name          K  *           * school_name        K  *
* user_last_name     Câ†‘ *           * school_city        Câ†‘ *
* user_id            Câ†“ *           * school_level          *
* user_address          *  --> Q2   *************************
* school_name           *     (Find
*************************      school by name)
```

Q1 ve Q2 application flow'una gÃ¶re sÄ±rasÄ± ile yapÄ±lacak isteklerdir.

YukarÄ±daki tablo notasyonuna gÃ¶re join'leme kÄ±smÄ± application client tarafÄ±nda yapÄ±lmasÄ± beklenir.

K primary key'i temsil eder.

Câ†‘ clustering key'i temsil eder. ASC sorting yapar.

## ğŸ“Œ partitioning key vs clustering key

Simple bir tablo yaratmaya bakalÄ±m:

```sql
create table simple_table (
    TC_NO int PRIMARY KEY, -- partitioning key: TC_NO
    TELEFON int
);
```

Åimdi ise __composite key (âŸ· compound key âŸ· bileÅŸik anahtar)__ olan tablo yaratalÄ±m:

```sql
create table table1 (
      TC_NO int,
      TELEFON int,
      DOGUM_YILI int,
      PRIMARY KEY(TC_NO, TELEFON) -- partitioning key: TC_NO, Clustering Key: TELEFON
);
```

"PRIMARY KEY" parantezinin iÃ§indeki ilk kÄ±sÄ±m __partitioning key__'dir. Where koÅŸullarÄ± ile data Ã§ekerken bu key ÅŸarttÄ±r. Ã‡Ã¼nkÃ¼ Cassandra node'larÄ± artÄ±k partitioning key'e gÃ¶re dÄ±ÅŸarÄ±dan aramaya hizmet vermektedir.

Ã–rneÄŸin; TC hash'leri 0-999 arasÄ±ndaki satÄ±rlar node1'de, 1000-1999 arasÄ± satÄ±rlar node2'de bulunuyor. Her node toplamda eÅŸit sayÄ±da satÄ±rlarÄ± barÄ±ndÄ±rÄ±yor. Bu ÅŸekilde denge saÄŸlanmÄ±ÅŸ oluyor. Cassandra bir satÄ±ra ait tÃ¼m primary key'lerin birlikte hash'ini alÄ±r ve buna gÃ¶re node'lara daÄŸÄ±tÄ±r. Hash Ã§Ä±ktÄ±sÄ± standart olduÄŸundan (belli boyutta uzunluÄŸu, Ã§Ä±ktÄ±daki olabilecek karakterler kÃ¼mesi/Ã§eÅŸitliliÄŸi...), her node'a daÄŸÄ±tma iÅŸlemini kÃ¼meleyip/mapping yapabilir. (Cassandra hash algoritmasÄ± olarak non-cryptographic hash metotlarÄ± kullanÄ±lÄ±r)

NoSQL'ler bilgiyi binlerce node'a daÄŸÄ±tÄ±r. Bu durumda bir sorgu gerektiÄŸinde tÃ¼m bilginin binlerce node'da aranmasÄ± gerekir. Bunu hÄ±zlandÄ±rmanÄ±n tek yolu ÅŸÃ¶yle bir sorgu atabilmektir:

```sql
Select * from USERS where user_name = 'Ahmet' and node_id = new_york_9
```

YukarÄ±daki sorguda data'nÄ±n new_york'taki 9'uncu instance'ta (node'da) olduÄŸunu bilerek sorgu atmÄ±ÅŸ olduk. Oysa bunu bilmemiz imkansÄ±z. Ä°ÅŸte NoSQL sistemler tablodaki bir veya birden fazla sÃ¼tunu bizden tabloyu oluÅŸtururken Ã¶zellikle istiyor. bu Ã¶zel sÃ¼tunlardaki her satÄ±rÄ±n hash'i alÄ±nÄ±yor. her hash kÃ¼mesi fiziksel olarak 1 instance/node'a kaydediliyor. yukarÄ±daki TC Ã¶rneÄŸindeki gibi. DolayÄ±sÄ± ile artÄ±k bilgiye eriÅŸmek istediÄŸimizde ilgili node'un nerede olduÄŸunu ilgili sÃ¼tunun hash'inden anÄ±nda bulabiliyoruz. DolayÄ±sÄ± ile milyonlarca node bile olsa anÄ±nda sonucu bulabiliyoruz. Temel konseptte birÃ§ok NoSQL bilgiyi bu ÅŸekilde distributed olarak paylaÅŸtÄ±rÄ±yor ve bu mekanizmaya __Distributed Hash Table (âŸ· DHT)__ deniliyor.

YukarÄ±daki tablo oluÅŸturma SQL'inde "Primary key" 2 kÄ±sÄ±mdan oluÅŸur. ilk kÄ±sÄ±m "__partition key__", ikinci kÄ±sÄ±m "__clustering key__" olarak deÄŸerlendirilir. "partition key" birden fazla olduÄŸunda parantez iÃ§erisine alÄ±nmalÄ±dÄ±r. clustering key'lere gÃ¶re sorting yapabiliriz anlamÄ±na gelir. her satÄ±r clustering key'lere gÃ¶re fiziksel olarak sorting yapÄ±larak tutulur. Ã–rneÄŸin TC'si 0-999 arasÄ±ndaki satÄ±rlar node1'de olsun. node1 iÃ§erisinde fiziksel olarak TELEFON numarasÄ±na gÃ¶re sorting yapÄ±lmÄ±ÅŸ ÅŸekilde tÃ¼m satÄ±rlar tutulur.

DiÄŸer Ã¶rnekler:

```sql
create table table2 (
      TC_NO int,
      TELEFON int,
      DOGUM_YILI int,
      OLUM_YILI int,
      PRIMARY KEY((TC_NO, TELEFON), DOGUM_YILI) -- partitioning key: TC_NO ve TELEFON, Clustering Key: DOGUM_YILI
);
```

```sql
create table table3 (
      TC_NO int,
      TELEFON int,
      DOGUM_YILI int,
      OLUM_YILI int,
      PRIMARY KEY(TC_NO, TELEFON, DOGUM_YILI) -- partitioning key: TC_NO, Clustering Key: TELEFON ve DOGUM_YILI
);
```

Where koÅŸullarÄ±nda partition key'deki tÃ¼m elemanlarÄ± girmek zorunludur. Yani table2'de TC_NO ve TELEFON (ikisi ile birlikte) ile where sorgusu yapmak zorundayÄ±z. table2 iÃ§in Ã¶rnek vermeye devam edersek; Where koÅŸuluna, ek olarak; DOGUM_YILI da kullanÄ±labilir. fakat zorunlu deÄŸildir. DOGUM_YILI sort'landÄ±ÄŸÄ± iÃ§in hÄ±zlÄ± dÃ¶nÃ¼ÅŸ saÄŸlayacaktÄ±r.

Partitioning konusu tÃ¼m NoSQL'ler iÃ§in geÃ§erli olduÄŸu iÃ§in Ã§ok Ã¶nemli. Data'yÄ± farklÄ± node'lara bÃ¶lmek aÅŸaÄŸÄ±daki gibi avantajlar saÄŸlayabilir:

- sadece belli partition'daki data'nÄ±n kolayca replika edilmesi (diÄŸer node'lardaki partition'lara eriÅŸim durduÄŸu zaman da devam edebilmesi)
- sadece belli partition'daki data'lara sorgu atÄ±lmasÄ± (performans avantajÄ±)

## ğŸ“Œ terminoloji

Cassandra dÃ¶kÃ¼mantasyonlarÄ±nda direk olarak "sharding" yoktur. Fakat arka planda zaten sharding yapÄ±labilmektedir.

Fakat genel olarak DB dÃ¼nyasÄ±nda terminoloji ÅŸu ÅŸekildedir:

"__partition key__" and "__shard key__" and "__sharding key__" and "__distribution key__" terms are equivalent. https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/database-sharding-explained-in-plain-english/ba-p/868774 1st subtitle.

"shard number" ise totaldeki shard sayÄ±sÄ±nÄ± ifade eder. "shard key" ile karÄ±ÅŸtÄ±rÄ±lmamalÄ±dÄ±r. https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/database-sharding-explained-in-plain-english/ba-p/868774 2nd subtitle.

## ğŸ“Œ Consistent hashing

NoSQL'lerde kullanÄ±lan bir tekniktir.

Klasik HashMap'lerde store edilecek data hash'lenir ve module iÅŸlemine tabi tutulur. Bu iÅŸlemin sonucunda data'nÄ±n nereye store edileceÄŸi bulunmuÅŸ olur. Yani; module iÅŸleminin sonucunda, node'un index'ini bulmuÅŸ oluruz.

```text
node-index = mod( hash(key-of-data) , total-index-count)
```

Ã–rnek:

4 node'umuz var. TC: 1111119999 olan satÄ±rÄ±n hangi node'da olduÄŸunu belirleyelim:

```text
43 = hash(1111119999)
3 = mod(43 , 4)
```

Fakat bu yÃ¶ntem ile bir node dÃ¼ÅŸtÃ¼ÄŸÃ¼nde veya yeni bir node eklendiÄŸinde, o node'a ait data'larÄ±n tÃ¼mÃ¼nÃ¼ taÅŸÄ±mak zorunda kalacaÄŸÄ±z. Bu durum istenen bir durum deÄŸildir. Bu data taÅŸÄ±ma iÅŸleminin bir ÅŸekilde en aza indirilmesi gerekmektedir. Bu sebeple yukarÄ±daki yÃ¶nteme alternatif olarak "Consistent hashing" tekniÄŸi geliÅŸtirilmiÅŸtir.

Consistent hashing tekniÄŸinde, her node belli bir aralÄ±ktaki data'larÄ± alÄ±r. Tabi data'larÄ±n hash'li halinden bahsettiÄŸimiz iÃ§in dÃ¼zenli bir Ã§Ä±ktÄ± kÃ¼mesi sÃ¶z konusudur. Person-National-Identifier'Ä± hash'lediÄŸimizi varsayalÄ±m. Person-National-Identifier'Ä± hash'lenmiÅŸ hallerinin, 1000-3000 arasÄ± olanlar N1 node'unda, 3000-5000 arasÄ± olanlar N2 node'unda olacak gibi... Yani:

- 1000-3000 arasÄ± olanlar N1 node'unda
- 3000-5000 arasÄ± olanlar N2 node'unda

EÄŸer araya yeni bir node eklenirse, sadece yakÄ±n deÄŸerlerdeki bir kÄ±sÄ±m data, yeni node'a eklenecek ve data farklÄ± 2 node'a bÃ¶lÃ¼ÅŸtÃ¼rÃ¼lecek. Ã–rneÄŸin son durum ÅŸÃ¶yle olabilir:

- 1000-2000 arasÄ± olanlar N1 node'unda
- 2000-4000 arasÄ± olanlar N3 node'unda --> Bu node'a 1000 satÄ±r N1'den, 1000 satÄ±r'da N2'den taÅŸÄ±nÄ±r. Yani iÅŸ bÃ¶lÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.
- 4000-5000 arasÄ± olanlar N2 node'unda

"Consistent hashing" genel bir terimdir. Bu tekniÄŸin birÃ§ok implementasyonu vardÄ±r. YukarÄ±da anlatÄ±lan "ring-based hashing" dir ve detaylÄ± ÅŸekilde anlatÄ±lmamÄ±ÅŸtÄ±r.

## ğŸ“Œ blob

Cassandra'nÄ±n byte array data tipidir. Java'da bu data mapping yapÄ±ldÄ±ÄŸÄ±nda ByteBuffer olarak tutulmalÄ±dÄ±r.

## ğŸ“Œ Cassandra write consistency levels

GÃ¼ncel `Cassandra`'da, `consistency level` `CQL` komutunda verilemiyor. `CQL`'deki `WITH CONSISTENCY` kullanÄ±mdan kaldÄ±rÄ±ldÄ±. `consistency level` programatik olarak driver seviyesinde, yada sunucu tarafta cluster veya data center seviyesinde yada per-operation basis (for example insert or update) seviyesinde ayarlanabiliyor.

| Level        | Description                                                                                                                                                                                                                                                                                                        | Usage                                                                                                                                                                                                                                                                                                                                          |
|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ALL          | A write must be written to the commit log and memtable on all replica nodes in the cluster for that partition key.                                                                                                                                                                                                 | Provides the highest consistency and the lowest availability of any other level.                                                                                                                                                                                                                                                               |
| EACH_QUORUM  | Strong consistency. A write must be written to the commit log and memtable on a quorum of replica nodes in all data center.                                                                                                                                                                                        | Used in multiple data center clusters to strictly maintain consistency at the same level in each data center. For example, choose this level if you want a read to fail when a data center is down and the QUORUM cannot be reached on that data center.                                                                                       |
| QUORUM       | A write must be written to the commit log and memtable on a quorum of replica nodes.                                                                                                                                                                                                                               | Provides strong consistency if you can tolerate some level of failure.                                                                                                                                                                                                                                                                         |
| LOCAL_QUORUM | Strong consistency. A write must be written to the commit log and memtable on a quorum of replica nodes in the same data center as the coordinator node. Avoids latency of inter-data center communication.                                                                                                        | Used in multiple data center clusters with a rack-aware replica placement strategy, such as NetworkTopologyStrategy, and a properly configured snitch. Use to maintain consistency locally (within the single data center). Can be used with SimpleStrategy.                                                                                   |
| ONE          | A write must be written to the commit log and memtable of at least one replica node.                                                                                                                                                                                                                               | Satisfies the needs of most users because consistency requirements are not stringent.                                                                                                                                                                                                                                                          |
| TWO          | A write must be written to the commit log and memtable of at least two replica nodes.                                                                                                                                                                                                                              | Similar to ONE.                                                                                                                                                                                                                                                                                                                                |
| THREE        | A write must be written to the commit log and memtable of at least three replica nodes.                                                                                                                                                                                                                            | Similar to TWO.                                                                                                                                                                                                                                                                                                                                |
| LOCAL_ONE    | A write must be sent to, and successfully sent ACK by, at least one replica node in the local data center.                                                                                                                                                                                                     | In a multiple data center clusters, a consistency level of ONE is often desirable, but cross-DC traffic is not. LOCAL_ONE accomplishes this. For security and quality reasons, you can use this consistency level in an offline data center to prevent automatic connection to online nodes in other data centers if an offline node goes down. |
| ANY          | A write must be written to at least one node. If all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff has been written. If all replica nodes are down at write time, an ANY write is not readable until the replica nodes for that partition have recovered. | Provides low latency and a guarantee that a write never fails. Delivers the lowest consistency and highest availability.                                                                                                                                                                                                                       |
| SERIAL       | Achieves linearizable consistency for lightweight transactions by preventing unconditional updates.                                                                                                                                                                                                                | You cannot configure this level as a normal consistency level, configured at the driver level using the consistency level field. You configure this level using the serial consistency field as part of the native protocol operation. See failure scenarios.                                                                                  |
| LOCAL_SERIAL | Same as SERIAL but confined to the data center. A write must be written conditionally to the commit log and memtable on a quorum of replica nodes in the same data center.                                                                                                                                         | Same as SERIAL. Used for disaster recovery. See failure scenarios.                                                                                                                                                                                                                                                                             |

### ğŸ“ŒğŸ“Œ quorum

kelime anlamÄ±: yeterli Ã§oÄŸunluk

Bu deÄŸer ÅŸu ÅŸekilde hesaplanÄ±yor: (sum_of_replication_factors / 2) + 1
