# CONTAINERS AND VM

IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII

IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII

## ğŸ“Œ Network drivers

## ğŸ“Œ VirtualBox network drivers

`network interface` konusu baÅŸka baÅŸlÄ±kta anlatÄ±lÄ±yor.

1 `guest`'e 1'den fazla network baÄŸdaÅŸtÄ±rÄ±cÄ±sÄ± eklenebilir.

`VirtualBox` network baÄŸdaÅŸtÄ±rÄ±cÄ±sÄ± opsiyonlarÄ±:

### ğŸ“ŒğŸ“Œ Not attached

network cihazÄ± takÄ±lÄ±dÄ±r. ama kablosu takÄ±lÄ± deÄŸildir.

### ğŸ“ŒğŸ“Œ NAT

`subnet` oluÅŸturmaz.

`quest` dÄ±ÅŸ dÃ¼nyaya Ã§Ä±kmak isterse, `VM` driver tarafÄ±ndan `NAT` iÅŸlemine tabi tutulur.

`guest`'in aldÄ±ÄŸÄ± `IP`, private bir `IP`'dir.

`Host` dÄ±ÅŸÄ±ndan bu `VM`'e eriÅŸmek iÃ§in `port forwarding` yapmak ÅŸarttÄ±r.

### ğŸ“ŒğŸ“Œ NAT Network (âŸ· NAT Service)

`subnet` kurar. bu aÄŸa baÄŸlÄ± her `quest` birbirini gÃ¶rebilir. `quest` bu `subnet`'ten `IP` alÄ±r.

`quest` dÄ±ÅŸ dÃ¼nyaya Ã§Ä±kmak isterse, `VM` driver tarafÄ±ndan `NAT` iÅŸlemine tabi tutulur.

### ğŸ“ŒğŸ“Œ bridge networking

`host`'un baÄŸlÄ± olduÄŸu `DHCP`'den yeni bir gerÃ§ek `IP` alÄ±r.

Bunu yapabilmek iÃ§in random `MAC` adresi Ã¼retiyor. BazÄ± host `OS`'ler veya donanÄ±mlar bunu desteklemeyebilir.

### ğŸ“ŒğŸ“Œ Host-only networking

sanal bir aÄŸ kartÄ± yaratÄ±r. bu sanal aÄŸ kartÄ± iÃ§erisinde istediÄŸimiz kadar `VM`'i birbiri ile iletiÅŸime geÃ§irebilir. dÄ±ÅŸ dÃ¼nya ile iletiÅŸimde olmazlar.

opsiyonel olarak, bu `subnet`'te `DHCP` sunucuda yaratÄ±lÄ±r. (`VirtualBox` ayarlarÄ±ndan).

### ğŸ“ŒğŸ“Œ Internal networking

`Host-only networking` yÃ¶ntemi ile apaynÄ±dÄ±r. sadece sanal bir kart yaratÄ±lmaz. pnun yerine fiziksel network kartÄ± kullanÄ±lÄ±r. fakat internete Ã§Ä±kmazlar. yazÄ±lÄ±m tarafÄ±nda 1 kart fakat 2 network interface'i oluÅŸur.

sanal bir kart yaratÄ±lmamasÄ±; var olan bir fiziksel kartÄ±n kullanÄ±lmasÄ±na sebep olur. BÃ¶ylece `Wireshark` gibi bir uygulama ile `Internal networking`'e baÄŸlÄ± `VM`'lerin network trafiÄŸini izleyebiliriz.

### ğŸ“ŒğŸ“Œ Cloud networking

`guest`'in uzaktaki bir subnet'teymiÅŸ gibi davranabilmesi saÄŸlanÄ±r.

### ğŸ“ŒğŸ“Œ Generic networking

`Virtualbox` driver eklentilerinin Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.

## ğŸ“Œ ("host-only networking" and "internal networking") vs ("NAT" and "NAT Network")

`host-only networking` and `internal networking` private network'ten dÄ±ÅŸarÄ±ya `VirtualBox`'un engine'inde olan sanal bir `Switch` cihazÄ± simÃ¼lasyonu ile Ã§Ä±karken, `NAT` and `NAT Network` yine `VirtualBox`'un yarattÄ±ÄŸÄ± sanal bir `router` ile dÄ±ÅŸarÄ±ya Ã§Ä±kar.

## ğŸ“Œ Docker network drivers

### ğŸ“ŒğŸ“Œ none

network cihazÄ± hiÃ§ yok gibidir container iÃ§inde.

### ğŸ“ŒğŸ“Œ host

container kendi IP'sini almaz. host ile aynÄ± network'Ã¼ kulanÄ±r.

### ğŸ“ŒğŸ“Œ bridge

default.

sanal interface ve `subnet` yaratÄ±lÄ±r.

container'a bu `subnet`'ten bir `IP` atanÄ±r.

her `subnet`'in bir adÄ± var. bunu parametre olarak veririz. default `subnet` adÄ±: `bridge`'dir.

sadece aynÄ± `subnet`'te olan container'lar birbirleri ile haberleÅŸebilir.

### ğŸ“ŒğŸ“Œ overlay

kelime anlamÄ±: kaplama (Ã¶rtÃ¼).

Bunun iÃ§in `docker` engine'in diÄŸer uzak makinadaki `docker` engine ile `Docker Swarm` gibi bir araÃ§la baÄŸlanmÄ±ÅŸ olmasÄ± ÅŸarttÄ±r.

Uzak network'teki container'lar birbirleri ile aynÄ± `subnet`'teymiÅŸ gibi davranÄ±lar.

### ğŸ“ŒğŸ“Œ macvlan

`host`'Ä±n network'Ã¼nden kendisine yeni `IP` alÄ±r. Yani `VM`'deki `bridge` gibi Ã§alÄ±ÅŸÄ±r.

`host`'un bulunduÄŸu network'Ã¼n bilgilerini vermek zorundayÄ±z.

```sh
docker network create -d macvlan \
      --subnet="172.16.86.0/24" \
      --gateway="172.16.86.1" \
      -o parent="eth0" \
      "pub_net"
```

parent olarak ne vereceÄŸimiz Ã§alÄ±ÅŸma mode'u belirleyecektir:

#### ğŸ“ŒğŸ“ŒğŸ“Œ bridge mode

direk olarak gerÃ§ek fiziksel kartÄ±mÄ±z Ã¼zerinden Ã§alÄ±ÅŸÄ±r. bu sebeple gerÃ§ek fiziksel arayÃ¼zÃ¼mÃ¼zÃ¼ parent'a yazmak zorundayÄ±z.

#### ğŸ“ŒğŸ“ŒğŸ“Œ 802.1q trunk bridge mode

sanal yaratÄ±lan bir interface Ã¼zerinden fiziksel karta baÄŸlanÄ±r. bÃ¶ylece host'un arayÃ¼zÃ¼nde olan routing veya filter'lara takÄ±lmamÄ±ÅŸ oluruz.

parent'a `eth0.50` gibi `Docker` engine'in belirttiÄŸi ÅŸekilde isim vermeliyiz.

### ğŸ“ŒğŸ“Œ IPvlan

container'lar `host`'un network'Ã¼ne baÄŸlanÄ±r. ama:

- `host` ile aynÄ± `MAC` adresi kullanÄ±r.
- sanal bir `subnet` oluÅŸturulur.
- her container bu `subnet`'te `IP` alÄ±r (ama host `DHCP` vermez bunu. `Docker` engine verir.)

ArtÄ±k dÄ±ÅŸ dÃ¼nyaya istek atarsak ve alÄ±rsak, `OS` kime yÃ¶nlendireceÄŸini bilir.

### ğŸ“ŒğŸ“Œ Docker plugin

3Ã¼ncÃ¼ parti network driver'lar.

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

## ğŸ“Œ Virtual machine

## ğŸ“Œ Hypervisor (âŸ· virtual machine monitor âŸ· VMM)

VM yÃ¶netimini Ã§alÄ±ÅŸtÄ±ran yazÄ±lÄ±mlara verilen genel isimdir.

`Hypervisor`, run edildiÄŸi katmana gÃ¶re 2'ye ayrÄ±lÄ±r:

### ğŸ“ŒğŸ“Œ Type-1 hypervisor (âŸ· native hypervisor âŸ· bare-metal hypervisor)

`VM`'de Ã§alÄ±ÅŸan `OS`, donanÄ±m ile direk temastadÄ±r.

Ã–rnek: `Microsoft` `Hyper-V` (Linux'u da desteklemektedir), `Xen`, `VMware ESXi (âŸ· old-name:VMware ESX)`

### ğŸ“ŒğŸ“Œ Type-2 hypervisor (âŸ· hosted hypervisor)

`VM`'de Ã§alÄ±ÅŸan `OS`, donanÄ±m ile direk temasta deÄŸildir. Arada `host` `OS` vardÄ±r.

Ã¶rnek: `VirtualBox`, `QEmu (âŸ· Quick Emulator)`

## ğŸ“Œ DonanÄ±m sanallaÅŸtÄ±rma (âŸ· platform sanallaÅŸtÄ±rma)

kurulan `OS`'lara donanÄ±mlarÄ± paylaÅŸtÄ±rma iÅŸlemidir. `Hypervisor`'ler bu paylaÅŸtÄ±rma iÅŸlemini yÃ¶netir. `Hypervisor`'ler donanÄ±mlarÄ± paylaÅŸtÄ±rma iÅŸlemlerine gÃ¶re 3'e bÃ¶lÃ¼nÃ¼rler:

- `Tam SanallaÅŸtÄ±rma`: tamamiyle sanal olarak bir donanÄ±m yaratÄ±lÄ±r ve `OS`'lara bu donanÄ±m kullandÄ±rtÄ±lÄ±r.

- `Para-virtualization`: gerÃ§ek donanÄ±mlar direk olarak `OS`'lara baÄŸlanÄ±r. sanal donanÄ±m hiÃ§ yoktur.

- `KÄ±smi SanallaÅŸtÄ±rma`: hibrit yÃ¶ntemdir. `OS`'lara bazÄ± donanÄ±mlar sanal verilirken, bazÄ± donanÄ±mlar sanal yaratÄ±lÄ±p verilir.

## ğŸ“Œ KVM (âŸ· Kernel-based Virtual Machine)

- Ã¶zel bir isimdir.
- `KVM`, `Linux` modÃ¼lÃ¼dÃ¼r.
- `Hypervisor`'dÃ¼r.
- `type-1`'dir.

## ğŸ“Œ Parallels Desktop (âŸ· Parallels Desktop for Mac)

`MacOS` iÃ§in `Hypervisor`.

## ğŸ“Œ yazÄ±lÄ±m sanallaÅŸtÄ±rma

`Docker` gibi uygulamalara verilen genel isimdir.

## ğŸ“Œ Intel VT-x vs AMD-V

SanallaÅŸtÄ±rma teknolojileri iÃ§in `CPU`'larÄ±n yapmÄ±ÅŸ olduÄŸu Ã¶zel teknoloji isimleri.

## ğŸ“Œ Nested virtualization

`VM` iÃ§inde `VM` Ã§alÄ±ÅŸtÄ±rmaktÄ±r.

## ğŸ“Œ ThinApp

- `VMware` firmasÄ±nÄ±n uygulamasÄ±.
- yazÄ±lÄ±m sanallaÅŸtÄ±rmasÄ± yapar.

## ğŸ“Œ EmÃ¼latÃ¶r (âŸ· Ã¶ykÃ¼nÃ¼cÃ¼ âŸ· emulator) vs simulator (âŸ· simÃ¼latÃ¶r)

`emulator`; Ã§alÄ±ÅŸtÄ±rÄ±lacak gerÃ§ek executable'Ä± run eder. bunu yaparken, her step'i (en ufak Ã§alÄ±ÅŸtÄ±rma birimini) o anda gerÃ§ek Ã§alÄ±ÅŸan native CPU'nun anlayacaÄŸÄ± dile Ã§evirir.

`simÃ¼latÃ¶r` ile karÅŸÄ±laÅŸtÄ±rma doÄŸru olmaz. Ã‡Ã¼nkÃ¼ simÃ¼latÃ¶r daha Ã¼st seviyelidir. Ã–rneÄŸin; `Android` `emulator`, `android` `OS` Ã§alÄ±ÅŸtÄ±rmak iÃ§in yapÄ±lan yazÄ±lÄ±mdÄ±r. Ama `android` simÃ¼latÃ¶rÃ¼, `android` `OS` iÃ§inde gÃ¶mÃ¼lÃ¼ yÃ¼klÃ¼ gelen yazÄ±lÄ±mdÄ±r. Android simÃ¼latÃ¶rÃ¼ ÅŸunu yapar:

- 1- 

  `APK`, `Android` API'lerini Ã§aÄŸÄ±rdÄ±ÄŸÄ± zaman, o metodlarÄ± fake olarak implemente eder. Ã–rneÄŸin `google` `play services` varmÄ± diye sorgularsa `true` olarak dÃ¶nÃ¼ÅŸ yapar, oysa bÃ¶yle bir servis hiÃ§ yoktur.

- 2-

  Ã–rneÄŸin; `APK`, `for each` dÃ¶ngÃ¼sÃ¼ kurmuÅŸsa, 
  - bunu `emulator` gibi `CPU`'ya Ã§evirerekte yollayabilir,
  - direk te yolayabilir (eÄŸer `CPU` instruction'larÄ± zaten aynÄ± ise)

Ã¶rnekler:

- `Google`'Ä±n `AVD`'si aracÄ±lÄ±ÄŸÄ± ile, `android` hem native, hem de `emulator` modunda Ã§alÄ±ÅŸabilir.
  - `android` iÃ§in Ã¶zel image Ã¼retiliyor. bu image direk desktop intel `CPU`'larÄ±nÄ±n anlayacaÄŸÄ± dil iÃ§in derlenmiÅŸ. bu sebeple bu native Ã§alÄ±ÅŸÄ±yor. (yani `emulator` modunda deÄŸil).
  - `android`'in orjinal disk image'leri `ARM`'de Ã§alÄ±ÅŸacak ÅŸekilde tasarlandÄ±. bunlarÄ± android `emulator`'da Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±zda, `emulator` arka planda her iÅŸlemi anlÄ±k convert eder. bu kat ve kat daha yavaÅŸ Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar. (`emulator` modu)

- `Android` `TV` iÃ§in `emulator` Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±zda, `emulator`,
  - `TV` ortamÄ±nÄ±n sadece fiziksel ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ simÃ¼le eder,
  - ancak android'in `CPU` iÅŸlemlerini emÃ¼le eder.

- eÄŸitimlerde kullanÄ±lan uÃ§ak oyunlarÄ± simÃ¼latÃ¶rdÃ¼r. Ã¼zerinde farklÄ± paketler execute edilip edilmemesi onun sÄ±fatÄ±nÄ± deÄŸiÅŸtirmez. ama case'deki simÃ¼lasyon, gerÃ§ek hayatÄ±n simÃ¼lasyonu anlamÄ±nda kullanÄ±lÄ±r.

## ğŸ“Œ VirtualBox OSE (âŸ· VirtualBox open source edition)

`Virtualbox` 4Ã¼ncÃ¼ sÃ¼rÃ¼mden Ã¶nce kapalÄ± kaynak ve aÃ§Ä±k kaynaklÄ± sÃ¼rÃ¼m olarak daÄŸÄ±tÄ±lÄ±yordu. 4 Ã¼ncÃ¼ sÃ¼rÃ¼m ile birlikte ikisi birleÅŸtirildi. ve lisans gerektiren ek Ã¶zellikler; kapalÄ± kaynak eklentiler olarak indirilebiliyor.

## ğŸ“Œ VMware

`VMware`'in birÃ§ok farklÄ± Ã¼rÃ¼nÃ¼ mevcut. TÃ¼m Ã¼rÃ¼nler kapalÄ± kaynaktÄ±r.

## ğŸ“Œ VMware Workstation (âŸ· VMware Workstation Pro)

`VM` Ã§alÄ±ÅŸtÄ±rÄ±cÄ±sÄ±dÄ±r. lisans gerektiren versiyon.

## ğŸ“Œ VMware Workstation Player (âŸ· VMware Player)

KiÅŸisel kullanÄ±m iÃ§in Ã¼cretiz sunuluyor. `VMware Workstation`'dan farklÄ± bir executable olup, `PRO`'ya gÃ¶re daha az Ã¶zellik iÃ§eriyor.

## ğŸ“Œ VMware Fusion

`MacOS`'in altyapÄ±sÄ± farklÄ± olduÄŸundan, `MacOS` versiyonu farklÄ± build Ã¼retilmiÅŸtir. Build o kadar farklÄ± ki, farklÄ± bir Ã¼rÃ¼n ismi ile daÄŸÄ±tÄ±lmaktadÄ±r.

## ğŸ“Œ Virtual Machine Manager (âŸ· virt-manager) vs Gnome-Boxes vs Cockpit

tÃ¼mÃ¼ birbirine alternatif GUI uygulamalarÄ±dÄ±r.

`virt-manager` Ã¶zel bir uygulama ismidir.

`gnome-boxes` aÅŸÄ±rÄ± az Ã¶zellik iÃ§erir.

`virt-manager` native desktop GUI'si var ve artÄ±k geliÅŸtirilmiyor. onu yerine `Web UI` ile hizmet veren `Cockpit` geliÅŸtirilmeye baÅŸlandÄ±.

## ğŸ“Œ Vagrant

`Virtualbox`, `VMWare` gibi uygulamalarÄ±n iÃ§erisinde olan makinalarÄ±, `gitops` tarzÄ± yÃ¶netimini saÄŸlayan aÃ§Ä±k kaynaklÄ± bir komut satÄ±rÄ± uygulamasÄ±dÄ±r.

Ã¶rnek iÅŸlemler:

- multi `hypervisor`'Ã¼ tek bir API'den yÃ¶netebilmemizi saÄŸlar.
- `OS`'u hazÄ±r bulut image'lerden indirip aÃ§ar.
- `OS` run, stop eder.
- `OS` iÃ§inde bir user tanÄ±mlar.
- bu user'Ä±n iÃ§ine `SSH` komutu ile girip, istediÄŸimizi Ã§alÄ±ÅŸtÄ±rabilmemizi saÄŸlar.

## ğŸ“Œ Vagrant nasÄ±l guest OS'un iÃ§ine girip komut Ã§alÄ±ÅŸtÄ±rabiliyor

## ğŸ“ŒğŸ“Œ SSH

OS imaj dosyalarÄ± Ã¶zeldir ve iÃ§inde default `vagrant` isminde user ve `SSH` server yÃ¼klÃ¼ gelir ve TCP porttan default olarak dinlemeye baÅŸlar. Vagran buraya istek atar.

## ğŸ“ŒğŸ“Œ WinRM (âŸ· Windows Remote Management)

sadece `MS Windows` guest iÃ§in Ã§alÄ±ÅŸÄ±r.

`MS Windows` aÃ§Ä±ldÄ±ÄŸÄ±nda bir `TCP` portundan komut Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ±na izin verir.

## ğŸ“ŒğŸ“Œ DiÄŸerleri

Bu alt baÅŸlÄ±ktakileri `Vagrant`'Ä±n destekleyip desteklemediÄŸini bilmiyorum.

Ama bu alt baÅŸlÄ±ktakilerde host ile guest arasÄ±nda bir veri paylaÅŸÄ±mÄ± yapÄ±labilmesini saÄŸlÄ±yor.

### ğŸ“ŒğŸ“ŒğŸ“Œ guest tools

#### ğŸ“ŒğŸ“ŒğŸ“ŒğŸ“Œ PCI device

guest'e bir `PCI` kablosu takÄ±lÄ±r. VM manager buradan zaten guest'te yÃ¼rklÃ¼ olan guest tools yazÄ±lÄ±mÄ±na bilgi yollar ve alÄ±r.

#### ğŸ“ŒğŸ“ŒğŸ“ŒğŸ“Œ Backdoor I/O port

`CPU`'ya bilgi atarken ve alÄ±rken Ã¶zel komutlar Ã¼zerinden data alÄ±ÅŸveriÅŸi saÄŸlanÄ±r.

### ğŸ“ŒğŸ“ŒğŸ“Œ Cloud/Remote provisioning (cloud-init, metadata, vs.)

basit bir text config dosyasÄ± devoloper tarafÄ±nda doldurulur. bu config dosyasÄ± okunarak sistem kurulur veya boot olurken okunur.

`Cloud-init` veya `user-data` veya `metadata`: `OS` boot olurken Ã§alÄ±ÅŸÄ±r.

`Preseed` veya `Kickstart`: `OS` install olurken Ã§alÄ±ÅŸÄ±r.

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

## ğŸ“Œ VM vs app container

karÅŸÄ±laÅŸtÄ±rÄ±lmalarÄ± doÄŸru deÄŸil. biri `VM` container'Ä±, diÄŸeri uygulama container'Ä±.

container, `OS` Ã§ekirdeÄŸi iÃ§ermez. ortak kullanÄ±r. `VM` iÃ§erir.

- `VM`'in aÃ§Ä±lmasÄ± dakikalar alÄ±rken, container saniyeler alÄ±r.

- `VM` fully portable'dÄ±r. fakat container deÄŸildir. container, `OS`'u sanallaÅŸtÄ±rmÄ±yor. var olan `OS` Ã§ekirdeÄŸini kullanÄ±yor. bu sebeple baÅŸka makinaya atÄ±lan container, aÅŸaÄŸÄ±daki durumlarda farklÄ± tepkiler verebilir:

  - `host` `OS` Ã§ekirdeÄŸi farklÄ± sÃ¼rÃ¼m ise
  - `host` `OS` Ã§ekirdeÄŸi farklÄ± ise
  - `host` `OS` Ã§ekirdeÄŸin bazÄ± Ã¶zellikleri aktif ise, baÅŸka makinada aynÄ± Ã¶zellikler aktif olmayabilir (Ã¶rnek `Linux` kernel module yÃ¼klÃ¼ olmayabilir)
  - donanÄ±m farklÄ± ise (Ã¶zellikle donanÄ±ma baÄŸlÄ± kodlarda)

  Bu durumlar bazen `VM`'de de geÃ§erli olabilir.

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

## ğŸ“Œ app container vs system container (âŸ· OS Container) vs Pod vs init container vs sidecar container

### ğŸ“ŒğŸ“Œ terminoloji

`container` 1 process tetiklenerek baÅŸlatÄ±lÄ±r. EÄŸer bu process kapanÄ±rsa, `container` otomatik kapanÄ±r. Fakat ilgili process baÅŸka process'ler baÅŸlatabilir. Ama `container` asÄ±l ilk process'i takip eder. ona gÃ¶re kapanÄ±p kapanmayacaÄŸÄ±na karar verir.

EÄŸer biz birden fazla baÄŸÄ±msÄ±z process Ã§alÄ±ÅŸtÄ±racak isek ve process'lerin tÃ¼mÃ¼ kapanmadan `container` kapanmasÄ±n istiyorsak, o zaman `init process` kullanmak zorundayÄ±z.

- `app container` sadece bir process iÃ§erir. sadece ilk process baÅŸka process aÃ§abilir.

  `system container` ise aynÄ± `container` iÃ§inde birden fazla process iÃ§erebilir. bunu saÄŸlarken de `init process` kullanmak zorunda deÄŸiliz (istersek kullanabiliriz).

  `kaynak: https://containerjournal.com/features/system-containers-vs-application-containers-difference-matter/ "System Containers" baÅŸlÄ±ÄŸÄ± 1inci paragraf.`

  `kaynak: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_system_containers "System containers provide a way to containerize services that need to run before the Docker daemon is running".`

  `kaynak: https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction "Operating System Containers" baÅŸlÄ±ÄŸÄ±.`

- `system container` ile `OS Container` eÅŸ anlamda kullanÄ±lÄ±r. `kaynak: https://docs.jelastic.com/what-are-system-containers 1inci paragraf, ilk cÃ¼mle.`

- `pod` iÃ§inde birÃ§ok process var ama bu process'lerin her biri ayrÄ± `container`'larda Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor. bunlarÄ± manage eden Ã¶zel yapÄ±ya `pod` deniliyor.

  `pod` yapÄ±sÄ±na `pause container (âŸ· sandbox container âŸ· infra container)` denir.

  `K8s` ise pod yapÄ±sÄ±nÄ± `parent container` olarak isimlendirir.

  `pod` teknolojisi Ã§Ä±kÄ±nca ile `system container` terimi:

  - microservice dÃ¼nyasÄ±nda rafa kalktÄ±.
  - fakat farklÄ± durumlarda kullanÄ±lmaktadÄ±r (baÅŸka baÅŸlÄ±kta anlatÄ±lÄ±yor)

- `system container` platformlarÄ±:
  - `BSD jails`
  - `Linux vServer`
  - `Solaris Zones`
  - `LXD`
  
  `kaynak: https://docs.jelastic.com/what-are-system-containers son paragraf.`

- `init container` ve `sidecar container` `K8s`'in terminolojisinde vardÄ±r. bunlar multi process veya iÃ§indeki process'in nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ± ile ilgili termler deÄŸildir. Ã–rneÄŸin;
   
   - `init container`, ona iliÅŸkilendirilmiÅŸ olan bir container baÅŸlamadan Ã¶nce otomatik baÅŸlatÄ±lÄ±r.
   - `sidecar container`, her zaman iliÅŸkilendirilmiÅŸ olan container ile aynÄ± `Pod` iÃ§inde koÅŸmak zorundadÄ±r.

### ğŸ“ŒğŸ“Œ IPC Ã§eÅŸitleri

#### ğŸ“ŒğŸ“ŒğŸ“Œ network socket

fiziksel veya sanal network cihazÄ± yok ise bile, sanal olan `loopback` gibi `network interface` Ã¼zerinden yine haberleÅŸilebilir.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Message passing

Mesaj karÅŸÄ±ya sync olarak atÄ±lÄ±r ve direk atÄ±lÄ±r.

Ã–rnekler: `Java` `RMI`, `CORBA`, `DCOM`.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Message queue

simple `OS`-based local bus.

Examples:

- `POSIX.1-2001`
- `SYS V` (Used by `Unix`)

#### ğŸ“ŒğŸ“ŒğŸ“Œ shared memory
  
Share part of the `RAM` directly. Fastest.

example:

- `POSIX shared memory`
  
  `POSIX` standartÄ±nda olan Ã¶zel bir API ismidir. Bu API'ye uyan herhangi bir kÃ¼tÃ¼phane aracÄ±lÄ±ÄŸÄ± ile 2 process `OS`'a ortak `RAM` kullanmak iÃ§in haber verebilir.

  `nmap`, `Unix`'in sunduÄŸu bir system call. `POSIX shared memory` yapabilmek iÃ§in `nmap` system call'undan yararlanmak zorundayÄ±z.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Named Pipe

`Unix domain socket`'e alternatiftir ama daha basit bir yapÄ±dÄ±r.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Unix domain socket

network socket'i ile alakasÄ± yoktur.

file tekniÄŸi ile aynÄ±dÄ±r. sadece dosyanÄ±n tipi socket'tir.

`Java` example:

```java
File socketFile = new File("/tmp/mysocket.sock");
AFUNIXServerSocket server = AFUNIXServerSocket.bindOn(socketFile)
server.accept().getOutputStream().write("hello".getBytes());
```

Arka planda `TCP/IP` stack'i kullanÄ±lmaz.

network socket yerine tercih edilir. Ã§Ã¼nkÃ¼:

- network aÄŸÄ± olmayan gÃ¶mÃ¼lÃ¼ sistemlerde, `TCP/IP` modÃ¼lleri yÃ¼klÃ¼ olmayabilir.

- herkes network portuna herkes istek yapabilir. bu gÃ¼vensizdir. dÄ±ÅŸarÄ±ya kapatmak iÃ§in ekstra config gerekebilir.

- `loopback` iÅŸlemleri yavaÅŸtÄ±r.

#### ğŸ“ŒğŸ“ŒğŸ“Œ DBus (âŸ· D-Bus)

`DBus` Ã¼st seviyeli kalÄ±yor. Kernel deÄŸilde, user yetkileriyle Ã§alÄ±ÅŸabiliyor. `DBus` daemon'a istek atan client'lar `TCP` socket veya `Unix domain socket` kullanabilir.

sync veya `publish-subscribe` mantÄ±ÄŸÄ±nda iÅŸlem yapabilmemizi saÄŸlar.

`DBus` daemon'a istek atabilen birÃ§ok client var:
- `GDBus` (`GNOME`)
- `QtDBus` (`Qt` or `KDE`)
- `dbus-java`
- `libdbus` (reference implementation)

her yazÄ±lÄ±m, `DBus` aracÄ±lÄ±ÄŸÄ± ile dÄ±ÅŸarÄ±ya birÃ§ok servis aÃ§abilir. her servis ID'si tÃ¼m OS'ta tekil olmalÄ±dÄ±r. bu servislerin her birine `bus` denir. Ã¶rnek bir `bus`:

```text
/com/appname/account
```

yukarÄ±da `account` servisi var. bu servis iÃ§inde birÃ§ok metot olabilir.

#### ğŸ“ŒğŸ“ŒğŸ“Œ file

bir app bir dosyaya yazar diÄŸeri okur.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Memory-Mapped File

shared memory ile aynÄ±dÄ±r. ama dosya aynÄ± zamanda disk'te de tutulur. bÃ¶ylece kalÄ±cÄ±lÄ±k saÄŸlanÄ±r.

#### ğŸ“ŒğŸ“ŒğŸ“Œ process signal

bir process diÄŸer process'e bilgi atamaz ama basit Signal'ler yollayabilirler.

`PID namepaces`'e dayanÄ±r.

### ğŸ“ŒğŸ“Œ container teknolojilerinde IPC iÃ§in Ã§Ã¶zÃ¼mler

PaylaÅŸÄ±m iÃ§in 2 Ã§Ã¶zÃ¼m var:

- `pod` (mÃ¼mkÃ¼nse tercih edilmeli)
- `system container`

### ğŸ“ŒğŸ“Œ aynÄ± container'da birden fazla process aÃ§mak iÃ§in sebepler

#### ğŸ“ŒğŸ“ŒğŸ“Œ POD Ã§Ã¶zÃ¼mÃ¼ yeten durumlar

aÅŸaÄŸÄ±daki Ã¶rnekler iÃ§in `POD` Ã§Ã¶zÃ¼mÃ¼ yeterli. sadece ek `namespace` ayarÄ± gerekebilir.

not: `kubernetes` ve `podman`, `pod` iÃ§indeki `container`'lara farklÄ± `namespace`'leri paylaÅŸtÄ±rÄ±yorlar default'ta.

- `container` iÃ§inde `SSH` server aÃ§abilmek iÃ§in.

  `kaynak: https://docs.jelastic.com/what-are-system-containers 2inci paragraf.`

- `crond`

  `kaynak: https://docs.jelastic.com/what-are-system-containers 2inci paragraf.`

- `Syslogd`

  `kaynak: https://docs.jelastic.com/what-are-system-containers 2inci paragraf.`

- `sidecar` pattern'i uygulayabilmemiz iÃ§in.

  tÃ¼m `service mesh` teknolojileri buna baÄŸlÄ±dÄ±r.

  `kaynak: https://ahmet.im/blog/minimal-init-process-for-containers/`

#### ğŸ“ŒğŸ“ŒğŸ“Œ POD Ã§Ã¶zÃ¼mÃ¼ yetmeyen durumlar

- `container` iÃ§inde bir web tarayÄ±cÄ±sÄ± aÃ§mak istiyoruz. bÃ¶yle bir durumda container'Ä±n `system container`'Ä± olmasÄ± gerekir.

  bu iÅŸ `pod` ile de yapÄ±labilir. fakat `pod` iÃ§indeki neredeyse tÃ¼m `container`'larÄ± ortak `namespace` ile Ã§alÄ±ÅŸtÄ±rmÄ±ÅŸ oluruz. bu da zaten `system container` ile aynÄ± kapÄ±ya Ã§Ä±kar.

- `container` iÃ§inde sÃ¼rekli bir process'imiz aÃ§Ä±k kalmayabilir. Ã¶rneÄŸin `distrobox` ile development amaÃ§lÄ± local Ã§alÄ±ÅŸmalar yaparken.

  bu iÅŸ `pod` ile de yapÄ±labilir. fakat pod iÃ§inde `init process` kullanmak durumundayÄ±z. bu da zaten `system container` ile aynÄ± kapÄ±ya Ã§Ä±kar.

## ğŸ“Œ container runtime

- `LXC`

  `Linux Containers`'tan tÃ¼reyen Ã¶zel bir isimdir.

  `Linux` kernel modÃ¼lÃ¼ deÄŸildir.

- `Libcontainer`
- `Runc`
- `sysbox`

  `Runc`'nin fork'u.
  
  nested container support iÃ§in geliÅŸtirilmiÅŸtir.

  Ã–rneÄŸin `Docker`, `sysbox`'Ä± kullanarak nested container aÃ§abilir:

  ```sh
  docker run --runtime=sysbox-runc ubuntu
  ```

- `Crun`
- `Railcar`
- `Katacontainers`
- `Containerd`

  `Runc`'yi wrap ediyor.
  
  `ctr` komut satÄ±rÄ± client uygulamasÄ±dÄ±r.

yukarÄ±daki liste iÃ§in kaynak: `https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction "Container Runtime" baÅŸlÄ±ÄŸÄ±.`

TÃ¼m `container runtime`'ler, `Linux`'un iÃ§inde native gelen "`cgroups`" ve `Linux namespaces` Ã¶zelliklerini kullanarak Ã§alÄ±ÅŸÄ±r. Ek olarak `SELinux` ve `AppArmor` gibi Ã¶zelliklerden de sÄ±kÃ§a yararlanÄ±r.

## ğŸ“Œ Open Containers Initiative (âŸ· OCI)

`OCI`'nin referans implementasyonu `Runc`'dir.

`container engine`, `OCI` ile uyumlu ise, OCI'nin tÃ¼m implementasyonu olan `container runtime`'larÄ±nÄ± Ã§alÄ±ÅŸtÄ±rabilir.

## ğŸ“Œ Container engines

`LXC` gibi Ã¶zelliklerin son kullanÄ±cÄ± tarafÄ±ndan direk kullanÄ±lmasÄ± zordur Ã§Ã¼nkÃ¼ alt seviyelidir. Bu yÃ¼zden `Docker` ve `LXD` gbi projeler ortaya Ã§Ä±kmÄ±ÅŸtÄ±r.

| `container engine` name | company                                      | based on `container runtime`                                            |
|-------------------------|----------------------------------------------|-------------------------------------------------------------------------|
| `Docker`                | `Docker`                                     | SÄ±rasÄ± ile bu teknolojileri kullandÄ±: 1-`LXC` 2-`Libcontainer` 3-`Runc` |
| `LXD`                   | `canonical`                                  | `LXC` which is also develop by `Canonical`                              |
| `Rkt (âŸ· Rocket)`        | `Red Hat` (`CoreOS Team`)                    |                                                                         |
| `CRI-O`                 | `Cloud Native Computing Foundation (âŸ· CNCF)` | `Runc`                                                                  |
| `Podman`                |                                              | `Runc`                                                                  |

yukarÄ±daki liste iÃ§in kaynak: `https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction "Container Engine" 1inci paragraf.`

## ğŸ“Œ Container Orchestration

`container engine`'leri wrap eder.

- `K8s`
- `Istio` (based on `K8s`)
- `OpenShift` (based on `K8s`)
- `Swarm` (develop by `Docker`)
- `UCP (âŸ· Universal Control Plane)` (develop by `Docker`)
- `Mesos` (develop by `Apache`)

yukarÄ±daki liste iÃ§in kaynak: `https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction "Container Orchestration" baÅŸlÄ±ÄŸÄ± son paragraf.`

## ğŸ“Œ container runtime vs container engine

aÅŸaÄŸÄ±daki tÃ¼m bilgiler buradan kopyalanmÄ±ÅŸtÄ±r: `https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction "Container Runtime" baÅŸlÄ±ÄŸÄ± ve "Container Engine" baÅŸlÄ±ÄŸÄ±.`

The `container runtime` is responsible for:

- Consuming the container `mount` point provided by the `Container Engine` (can also be a plain directory for testing)
- Consuming the container metadata provided by the `Container Engine` (can be a also be a manually crafted `config.json` for testing)
- Communicating with the kernel to start containerized processes (clone `system call`)
- Setting up `cgroups`
- Setting up `SELinux` Policy
- Setting up `App Armor` rules

the `container engine` is responsible:

- Handling user input
- Handling input over an API often from a `Container Orchestrator`
- Pulling the Container Images from the Registry Server
- Expanding decompressing and expanding the container image on disk using a Graph Driver (block, or file depending on driver)
- Preparing a container `mount` point, typically on `CoW` storage (again block or file depending on driver)
- Preparing the metadata which will be passed to the container `Container Runtime` to start the Container correctly
  - Using some defaults from the container image (example: `ArchX86`)
  - Using user input to override defaults in the container image (example: `CMD`, `ENTRYPOINT`)
  - Using defaults specified by the container image (example: `SECCOM` rules)
- Calling the `Container Runtime`

## ğŸ“Œ Podman

`Pod Manager`'in kÄ±saltmasÄ±ndan Ã¶zel isim tÃ¼retilmiÅŸtir.

`Podman`; resmi sitesinden `daemonless container engine` olarak geÃ§iyor. Ã§Ã¼nkÃ¼ bir engine'e ihtiyacÄ± yok. zira kendisi (sadece client komut satÄ±rÄ± uygulamasÄ±) `Linux`'un iÃ§erisinde gelen teknolojileri kullanÄ±yor. DolayÄ±sÄ± ile ek kurulumda gerektirmiyor.

root yetkisiz de process aÃ§abiliyor (tabi kÄ±sÄ±tlÄ± Ã¶zellikler sunarak).

`Podman`, `pod` ve `container` olarak 2 farklÄ± seÃ§eneÄŸi de destekliyor.

## ğŸ“Œ Linux namespaces

bazÄ± kaynaklarda sadece `namespace` olarak geÃ§er. bu sebeple karÄ±ÅŸÄ±klÄ±ÄŸa sebep olabilir. zira `namespace` kelimesi birÃ§ok farklÄ± alanda kullanÄ±lmaktadÄ±r.

`Linux` Ã¼zerinde proseslerin hangi kaynaklarÄ± diÄŸer prosesler ile ortak kullanÄ±p kullanamayacaÄŸÄ±nÄ± belirleyen native `Linux` Ã¶zelliÄŸidir. Ã¶rnek `Linux namespaces` type'lar;

- __Mount namespaces__

  bir grup proses, spesifik bir dizini gÃ¶rebilirken, diÄŸer hiÃ§bir proses gÃ¶remeyebilir.

- `Network namespaces`

  birden fazla network arayÃ¼zÃ¼ (`eth`, `Wi-Fi` gibi) birden fazla network namespace'sine baÄŸlanÄ±r. network namespace'si iptable gibi routing ayarlarÄ±nÄ± iÃ§erir.

- `IPC namespaces`

  bir `IPC namespace`'si iÃ§indeki process, diÄŸer `IPC`'lere mesaj atamaz.

- __PID namespaces__

  process ID grubu oluÅŸturuluyor. bu ÅŸekilde, X pid namespace'indeki process'ler sadece birbirlerini gÃ¶rebiliyor.

- `uts namespaces (âŸ· UNIX Time-Sharing namespaces)`

  domain ve host name'leri okurken farklÄ± gruplar yaratabiliyoruz

- __User namespaces__

  bir process baÅŸlatÄ±ldÄ±ÄŸÄ±nda o process'in kendini X user'Ä± ve Y grubu altÄ±ndaymÄ±ÅŸ gibi sanmasÄ±nÄ± saÄŸlayabiliyoruz.

  Bu yetenek aynÄ± zamanda sandbox Ã¶zelliÄŸini de getirdi. Ã‡Ã¼nkÃ¼ yeni user ve grup aÃ§tÄ±ktan sonra artÄ±k burada Ã¶zgÃ¼rÃ¼z ve istediÄŸimiz namespace'i kapatabiliyoruz. yani; unshare komutu; eÄŸer yeni user-namespace yaratÄ±ldÄ±ysa, root yetki olmadan network'Ã¼ kapatabilir. fakat aynÄ± komut ile yeni user-namespace yaratÄ±lmamÄ±ÅŸ ise, network'Ã¼ ancak root yetkisi ile kapabiliriz.

- __time namespaces__

  her farklÄ± time nameSpace'inde olan process'ler sistem saatini farklÄ± algÄ±lÄ±yor. (bu namespace tipi 2020 ile Linux 5.6'ya geldi)

- __cgroup namespaces__ vs __cgroup (âŸ· control group)__

  __cgroup__ Linux Ã¼zerinde proseslerin CPU, memory, network gibi kaynaklardan ne kadar Ã¶ncelikli yararlanabileceÄŸi, yada limitli yararlanabilmeleri iÃ§in yÃ¶netimi saÄŸlayan Linux Ã§ekirdeÄŸinin native Ã¶zelliÄŸidir.

  __cgroup namespaces__ bir process'in cgroup grubuna dahil olduÄŸunu belirler.

## ğŸ“Œ Linux namespaces nasÄ±l Ã§alÄ±ÅŸÄ±r?

```text
/proc/<PROCESS_ID>/ns/
```

dizini altÄ±nda o process'in tÃ¼m namespace'leri ayrÄ± ayrÄ± link dosyalarÄ±dÄ±r:

```text
/proc/<PROCESS_ID>/ns/ipc
```

```text
/proc/<PROCESS_ID>/ns/mnt
```

Ä°ÅŸte bu dosyalar link olduÄŸundan, Ã¶rneÄŸin X namespace'sine link eder. Ä°ÅŸte o process'i, `Y` namespace'sine yÃ¶nlendirirsek o zaman container yapmÄ±ÅŸ oluruz.

`Linux namespaces`'leri `system call`'lar ile yapÄ±lÄ±r ve programatik olarak yapmak iÃ§in `system call` yapan kÃ¼tÃ¼phaneye ihtiyacÄ±mÄ±z vardÄ±r. Ã¶rneÄŸin `C`'de bunu yapmak iÃ§in `clone` fonksiyonu kullanÄ±labilir (baÅŸka baÅŸlÄ±kta anlatÄ±lÄ±yor).

## ğŸ“Œ sandbox vs container

container iÃ§inde uygulamalar Ã§alÄ±ÅŸtÄ±rabilmek iÃ§in (yani yeni namespace'ler yaratabilmek iÃ§in) root yetkisine ihtiyaÃ§ vardÄ±r. fakat sandbox iÃ§in root yetkisine gerek yoktur. Ã¶rnek Docker root yetkisi ister, fakat Chromium based tarayÄ±cÄ±lar iÃ§in sandbox iÃ§in user yetkisi yeterlidir.

Chromium-based tarayÄ±cÄ±lar sandbox kullanÄ±lÄ±r. BaÅŸka baÅŸlÄ±kta anlatÄ±lÄ±yor.

## ğŸ“Œ Idle process

`init process`'ten tamamen farklÄ± ve baÄŸÄ±msÄ±z programlardÄ±r.

`CPU` donanÄ±msal mimarisi gereÄŸi boÅŸta bekleyemez. SÃ¼rekli bir iÅŸlem yapmak zorundadÄ±r. Bu sebeple sonsuz dÃ¶ngÃ¼lÃ¼ bir program, `OS` tarafÄ±ndan arka planda (eÄŸer baÅŸka bir process Ã§alÄ±ÅŸmÄ±yorsa) Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r. Bu process'e genel olarak `idle process` denir. `MS Windows` dÃ¼nyasÄ±nda process'in Ã¶zel bir ismi vardÄ±r: `System idle process (âŸ· sistem boÅŸta iÅŸlemi)`.

`Linux`'ta process ID'si 0'dÄ±r.

## ğŸ“Œ init process

`OS` servis ve program yÃ¶netimi yazÄ±lÄ±mlarÄ±na verilen genel isimdir. sistem baÅŸlangÄ±cÄ±nda ilk baÅŸlayan iÅŸlemdir. bu sebeple `init process` olarak da adlandÄ±rÄ±lÄ±rlar. tÃ¼m servisler ve process'lerin yÃ¶netimini saÄŸlar. `OS` Ã§ekirdeÄŸi tarafÄ±ndan baÅŸlatÄ±lÄ±r.

process ID'leri 1'dir.

`init process`'ler bir `OS` iÃ§erisinde birkaÃ§ tane olabilir. bunun iÃ§in bazÄ± Ã¶rnek durumlar:

- her `container` iÃ§erisinde `init process` olabilir
- her `OS` user'Ä± iÃ§in ayrÄ± `init process` olabilir. `kaynak: kaynak: http://man.he.net/man8/init "NOTES" baÅŸlÄ±ÄŸÄ±, 1inci paragraf.`

Unix daÄŸÄ±tÄ±mlarda ilk geliÅŸtirilen `init process`, `System V` ismi ile daÄŸÄ±tÄ±lan daÄŸÄ±tÄ±mda kullanÄ±lÄ±yordu. Bunun Ã§Ä±kmasÄ± ile birlikte birÃ§ok `Linux` daÄŸÄ±tÄ±mÄ± dahi, buna uyumlu `init process` sistemleri ile daÄŸÄ±tÄ±lÄ±yordu. herkes kendi `init process`'ini yazÄ±yordu. bu sebeple bu tarz `init process`'ler iÃ§in `System V style init`, `traditional init` gibi terimler kullanÄ±lÄ±r.

ÅŸu anda piyasada birÃ§ok `init process` alternatifi mevcut: `systemd`, `upstart`

`Upstart` daha Ã§ok sÄ±ralÄ± bir aÃ§Ä±lÄ±ÅŸa aÄŸÄ±rlÄ±k verir. Ã¶rneÄŸin; update hizmeti, log servisine baÄŸlÄ± ise Ã¶nce log servisinin baÅŸlatÄ±lmasÄ± beklenir. hazÄ±r olunca update servisi baÅŸlar. oysa `systemd`'de iÅŸlemler paralel baÅŸlatÄ±lÄ±r. birbirlerine depend eden istekler beklemeye alÄ±nÄ±r. iÅŸlemler paralel baÅŸladÄ±ÄŸÄ±ndan `CPU` daha az boÅŸta kalÄ±r ve daha hÄ±zlÄ± aÃ§Ä±lÄ±ÅŸ gÃ¶zlenir.

`systemd` vs `Upstart` iÃ§in komut satÄ±rÄ± uygulamalarÄ± mevcut. bunlarÄ±n kullanÄ±m Ã¶rnekleri aÅŸaÄŸÄ±dadÄ±r:

| Operation                          | `Upstart` Command                 | `Systemd` command                 |
|------------------------------------|-----------------------------------|-----------------------------------|
| Start service                      | startÂ $job                        | systemctlÂ startÂ $unit             |
| Stop service                       | stopÂ $job                         | systemctlÂ stopÂ $unit              |
| Restart service                    | restartÂ $job                      | systemctlÂ restartÂ $unit           |
| See status of services             | initctlÂ list                      | systemctlÂ status                  |
| Check configuration is valid       | init-checkconfÂ /tmp/foo.conf      | systemd-analyze verify $unitFile  |
| Show job environment               | initctlÂ list-env                  | systemctlÂ show-environment        |
| Set job environment variable       | initctlÂ set-envÂ foo=bar           | systemctlÂ set-environmentÂ foo=bar |
| Remove job environment variable    | initctlÂ unset-envÂ foo             | systemctlÂ unset-environmentÂ foo   |
| View job log                       | catÂ /var/log/upstart/$job.log     | sudoÂ journalctlÂ -uÂ $unit          |
| tailÂ -f job log                    | tailÂ -fÂ /var/log/upstart/$job.log | sudoÂ journalctlÂ -uÂ $unitÂ -f       |
| Show relationship between services | initctl2dot                       | systemctlÂ list-dependenciesÂ --all |

not: `service` komutu bir `shell` script dosyasÄ±dÄ±r ve `init process` tÃ¼rÃ¼nÃ¼ bilmeden iÅŸlem yapabilmemize olanak saÄŸlayan bir wrapper'dÄ±r.

`Systemd` servisleri baÅŸlatÄ±rken hangi servisleri baÅŸlatacaÄŸÄ±na `target`'lar aracÄ±lÄ±ÄŸÄ± ile karar verir. Ã¶rneÄŸin sunucumuzda grafik arayÃ¼zÃ¼ hiÃ§ kurulmamÄ±ÅŸ ise, daha sonra `GUI` kurarsak ve varsayÄ±lan olarak `GUI` servislerinin aÃ§Ä±lmasÄ±nÄ± istiyorsak, default `target`'Ä± deÄŸiÅŸtirmeliyiz:

```sh
systemctl set-default graphical.target
```

output:

```text
Removed symlink /etc/systemd/system/default.target.
Created symlink from /etc/systemd/system/default.target to /usr/lib/systemd/system/graphical.target.
```

sistemde var olan target'larÄ± gÃ¶rebilmek iÃ§in:

```sh
systemctl list-units --type=target
```

## ğŸ“Œ Linux dÄ±ÅŸÄ±ndaki OS'larda Docker nasÄ±l Ã§alÄ±ÅŸÄ±yor

`container` yapÄ±sÄ± sadece `Linux`'ta vardÄ±r. fakat `MS Windows` ve `MacOS`, `container` yapÄ±sÄ± desteklemez. `Docker` uygulamasÄ± kurulduÄŸunda kendi bir sanal `Linux` makinesi kurmaktadÄ±r.

`Docker` var olan `OS`'un Ã§ekirdeÄŸini kullandÄ±ÄŸÄ± iÃ§in (ortada ikinci `OS` olmadÄ±ÄŸÄ± iÃ§in), `Docker`'Ä±n `Hypervisor` gibi `CPU` teknolojilerine baÄŸÄ±mlÄ±lÄ±ÄŸÄ± yoktur. en azÄ±ndan `Linux` iÃ§in bu ÅŸekilde. fakat `MS Windows`, 2016 yÄ±lÄ± sonrasÄ± sÃ¼rÃ¼mleri itibari ile container yapÄ±sÄ±nÄ± desteklemeye baÅŸlamÄ±ÅŸtÄ±r. fakat teknik altyapÄ±da donanÄ±ma baÄŸÄ±mlÄ±lÄ±k vardÄ±r. hyper-v tarzÄ± teknolojilere dayanmaktadÄ±rlar.

`MS Windows` base image olarak temelde ÅŸu 4Ã¼nÃ¼ resmi olarak sunuyor:

- `Nano Server` - is an ultra-light `MS Windows` offering for new application development. `.NET Core` kullanan uygulamalar iÃ§in bu image yeterlidir.
- `Server Core` - is medium in size for `MS Windows` `Server` apps.
- `Windows` is the largest image and has full `MS Windows` `API` support.
- `Windows Server` is slightly smaller than the `MS Windows` image, has full `MS Windows` `API` support, and allows you to use more server features.

BunlarÄ±n Ã¼stÃ¼ne kurulu `.Net` gibi birÃ§ok Ã¶zel, ayrÄ± ayrÄ± hazÄ±r image'ler de resmi olarak sunuluyor.

## ğŸ“Œ Docker Registry

internette bulunan hazÄ±r container repo'su hizmetlerinin genel adÄ±dÄ±r.

## ğŸ“Œ Docker Hub

`Docker` firmasÄ±nÄ±n sunduÄŸu resmi `Docker Registry`sinin ismidir.

## ğŸ“Œ Docker-compose

`YAML` konfigÃ¼rasyon dosyasÄ±nÄ± parametre olarak alÄ±r ve ona gÃ¶re docker komutlarÄ±nÄ± koÅŸar.

## ğŸ“Œ Docker-swarm

`swarm` TÃ¼rkÃ§e kelime anlamÄ±: sÃ¼rÃ¼

`container` orchestrator'dÄ±r.

## ğŸ“Œ docker-stack

`docker-swarm` kullanarak `docker`'larÄ± baÅŸlatan `docker-compose` alternatifidir.

## ğŸ“Œ Universal Control Plane (âŸ· UCP)

`docker-stack`'ten daha geliÅŸmiÅŸtir. ekstra sunduklarÄ±

- `Web UI`
- enterprise seviyesinde user ve role management
- arka planda `docker-swarm` veya `K8s` kullanabilir.

## ğŸ“Œ UCP-agent (âŸ· UCP node)

2 Ã§eÅŸittir:

- manager: `Web UI`'Ä± sunar ve worker'lara komut gÃ¶nderir.

- worker: Docker container'larÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r.

manager Ã¶zelliÄŸine sahip bir node'da isterse container Ã§alÄ±ÅŸtÄ±rÄ±r fakat bu pek tavsiye edilmez. sadece hÄ±zlÄ± test iÃ§in, deneme amaÃ§lÄ± kurulumlar yapÄ±lmasÄ± iÃ§in manager'larÄ±n container Ã§alÄ±ÅŸtÄ±rmasÄ± serbest bÄ±rakÄ±lmÄ±ÅŸtÄ±r.

manager ve worker node'larÄ± aynÄ± cluster'da birden fazla olabilir.

## ğŸ“Œ UCP bundle (âŸ· UCP client bundle)

`UCP` panelden download edilir. Bir `Zip` dosyasÄ±dÄ±r. iÃ§inde gerekli anahtarlar ve komut satÄ±rÄ± script'leri bulunur. bu script'ler kullanÄ±larak direk `Docker` yÃ¼klÃ¼ bir makinadan `UCP` `Web UI` panelindeymiÅŸ gibi komut satÄ±rÄ±ndan iÅŸlem yapÄ±labilmesini saÄŸlar.

## ğŸ“Œ service

service bir container instance'Ä± deÄŸil. Ã¶rneÄŸin yukarÄ±da 1 servise baÄŸlÄ± 5 instance oluÅŸturduk. istersek hemen 10 tane daha ekleyebiliriz. servise yaptÄ±ÄŸÄ±mÄ±z ayar, o servise baÄŸlÄ± tÃ¼m container'lara otomatik yapÄ±lmaktadÄ±r.

servis tanÄ±mÄ±mÄ±zÄ± gÃ¼ncellediÄŸimiz zaman o servisten tÃ¼remiÅŸ tÃ¼m container'lar kapanÄ±r ve yeni tanÄ±mlama ile tekrar aÃ§Ä±lÄ±rlar.

### ğŸ“ŒğŸ“Œ service types

Global service: her yeni worker node eklendiÄŸinde bu servislerden birer instance otomatik o worker'da aÃ§Ä±lmaktadÄ±r. firewall/antivirÃ¼s, monitor-agents gibi uygulamalarÄ±mÄ±z burada olabilir.

Replicated service: Ä°lgili servisten kaÃ§ adet instance'Ä±n Ã§alÄ±ÅŸacaÄŸÄ±na sizin karar verdiÄŸiniz servis tipidir.

## ğŸ“Œ Docker machine

ek bir komut satÄ±rÄ± uygulamasÄ±. bu uygulama ile yeni sanal `OS`'larÄ± kurabiliyoruz. bu ÅŸekilde container desteklemeyen `OS`'larda, kolayca `Linux` `VM` yaratarak onun Ã¼zerinde Docker Ã§alÄ±ÅŸtÄ±rmamÄ±zÄ± saÄŸlamaktadÄ±r. aynÄ± ÅŸekilde sadece lokalde deÄŸil, uzak bilgisayarlardaki `VM`'ler Ã¼zerinde de Ã§alÄ±ÅŸabilmemizi saÄŸlamaktadÄ±r.

`Docker machine`, `MS Windows` ve `Linux`'ta `VirtualBox` kullanÄ±rken, MacOS'te HyperKit kullanmaktadÄ±r.

`Docker machine` driver'larÄ± sayesinde `VMware`, `VirtualBox`, `Microsoft` `Azure`, `Amazon Web Services` gibi birÃ§ok farklÄ± bulut altyapÄ±sÄ±ndaki makina sistemlerine veya `VM` yÃ¶neticilerinde uzaktan iÅŸlem yapabiliyoruz.

## ğŸ“Œ Docker toolbox

`Docker engine` ve `Docker compose` ve `Docker machine` ve `Kitematic` uygulamasÄ±nÄ± bir arada sunan bir setup'tÄ±r.

`MS Windows` ve `MacOS` native destek saÄŸladÄ±klarÄ±ndan beri artÄ±k `Docker toolbox`'a olan destek kaldÄ±rÄ±ldÄ±. `MS Windows` ve `MacOS`'e kurulumlarda `Docker for Windows (âŸ· Docker Desktop for Windows)` veya `Docker for mac (âŸ· Docker Desktop for Mac)` kurulmasÄ± Ã¶neriliyor.

`Docker for Windows` ve `Docker for Mac`, `Kubectl` ve `Minikube` yazÄ±lÄ±mÄ±nÄ± da iÃ§inde bulundurmaktadÄ±r.

## ğŸ“Œ Kitematic

cross platform `Docker` client GUI'sidir.

## ğŸ“Œ Docker tooling

`Eclipse` iÃ§in `Docker` client eklentisidir. direk container'larÄ±n iÃ§ine komut yazabilmemizi de saÄŸlÄ±yor.

## ğŸ“Œ Docker container vs Docker image vs Dockerfile

`Dockerfile`, `Docker` image'i yaratmak iÃ§in Docker'Ä±n anlayacaÄŸÄ± tipte bilgiler/komutlar iÃ§eren dosyadÄ±r. bu dosya iÅŸlenerek (build iÅŸlemi) image image yaratÄ±lÄ±r.

Docker image, Dockerfile ile yaratÄ±lmÄ±ÅŸ run edilmeye hazÄ±r sanal pakettir.

Docker image run edildiÄŸinde, Docker container olur.

## ğŸ“Œ Dockerfile

Ã¶rnek Dockerfile:

```dockerfile
FROM Ubuntu 
# Docker container ilk aÃ§Ä±ldÄ±ÄŸÄ±nda, boÅŸ bir `file system` oluÅŸturur. Burada bir OS kullanmak gerekli. Bu ÅŸekilde temel kÃ¼tÃ¼phaneler `file system`'da bulunmuÅŸ olur.

ARGS PI = 3 
# container iÃ§inden bu deÄŸer okunamaz. komut satÄ±rÄ±ndan override edilebilir: "docker build --build-arg PI=4"

ENV ANDROID_HOME /my/dir
# yada ENV ANDROID_HOME=/my/dir. komut satÄ±rÄ±ndan override edilebilir: 
# docker run -e "ANDROID_HOME=/my/another/dir"

WORKDIR /directory1
# container iÃ§inde "cd /directory1" yapmÄ±ÅŸ olduk.

COPY /source/dir:/dest/dir 
# container dÄ±ÅŸÄ±ndaki bir dizini yada dosyayÄ± container iÃ§ine kopyalar

ADD /source/dir:/dest/dir 
# copy ile aynÄ±dÄ±r. ekstra olarak ADD; eÄŸer source URL ise onu indirir ve eÄŸer source arÅŸiv dosyasÄ± ise (tar, Zip) onu aÃ§Ä±p kopyalar.

RUN apt-get install NodeJS 
# sadece "docker build" komutu ile Ã§aÄŸrÄ±lÄ±r.

RUN echo "hello number $some_variable_name" 
# yada "hello number ${some_variable_name}".

CMD ["/usr/bin/node", "/var/www/app.js"] 
# sadece "docker run" komutu ile Ã§aÄŸrÄ±lÄ±r.

ENTRYPOINT ["/usr/bin/node", "/var/www/app.js1", "/var/www/app.js2"]
# CMD ile aynÄ± gÃ¶revi gÃ¶rÃ¼r. tek farkÄ± Docker'Ä± run ederken, ek parametreler alabilir. "Docker run myImage /var/www/app.js3" komutu ENTRYPOINT'e 3Ã¼ncÃ¼ parametreyi ekleyecektir.
```

## ğŸ“Œ userName/containerName vs containerName vs myhub.mysite.com:8081/myContainerName

Dockerfile'da her iki formatta da paket indirilebilir. userName olmadan indirilenler Docker tarafÄ±ndan resmi olarak tag'lenmiÅŸ anlamÄ±na gelmektedirler.

"Docker registry" iÃ§in hazÄ±r paket yazÄ±lÄ±mlar bulunmaktadÄ±r. yani; self-hosted ÅŸekilde kendimiz iÃ§in ayaÄŸa kaldÄ±rabiliriz.

## ğŸ“Œ Container OS

container'Ä± Ã§alÄ±ÅŸtÄ±ran her `OS`, host `OS`'tur. fakat bazÄ± `OS`'lar sadece container yÃ¼rÃ¼tmek iÃ§in Ã¶zel tasarlanmÄ±ÅŸtÄ±r. iÃ§inde gereksiz paketler yoktur. bunlara `Container OS` denir.

Ã¶rnek:

- `Snappy Ubuntu Core`
- `CoreOS`
- `RancherOS`
- `Project Atomic`
- `Photon`

## ğŸ“Œ Snappy Ubuntu Core (âŸ· Snappy Ubuntu âŸ· Ubuntu Core)

- sadece snap paket yÃ¶neticisi yÃ¼klÃ¼dÃ¼r.
- IoT cihazlarÄ±nda tercih edilmektedir.

## ğŸ“Œ Base OS (âŸ· base image OS)

container iÃ§inde koÅŸan `OS`'tur. `FROM` komutu ile `OS` baz alÄ±nÄ±rken, `OS`'un minimum olmasÄ± beklenmektedir. bu durum; hem bellekten kazandÄ±rÄ±r hem de baÄŸÄ±mlÄ±lÄ±klarÄ±n daha iyi belirlenmesini saÄŸlar. buradaki `OS`'larÄ± tamamen farklÄ± bir dizinde (`chroot` tarzÄ±) saklanmaktadÄ±r. `container`'da run edilen uygulamalar sadece burayÄ± gÃ¶rebilmektedirler. container iÃ§indeki uygulama, host `OS`'tan sadece `Linux` Ã§ekirdeÄŸini ortak kullanmaktadÄ±r. onun dÄ±ÅŸÄ±nda tÃ¼m kaynaklar mount edilen yeni dizin altÄ±ndadÄ±r.

bazen `base OS` terimi, `minimal OS` terimi yerine yanlÄ±ÅŸ kullanÄ±lmaktadÄ±r. ikisi farklÄ± ÅŸeyler. `minimal OS`, normal desktop sistemler iÃ§inde olabilir. `minimal OS`, sadece ek paketlerin olmadÄ±ÄŸÄ± anlamÄ±na gelmektedir.

bazen de `container OS` terimi `base OS` ile karÄ±ÅŸtÄ±rÄ±lmaktadÄ±r.

`Container` iÃ§erisinde sadece `OS` Ã§ekirdeÄŸini kullanan bir uygulama Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa, bu durumda `base OS`'a dahi ihtiyacÄ±mÄ±z olmaz. AslÄ±nda `Base OS`, iÃ§inde `OS` Ã§ekirdeÄŸi barÄ±ndÄ±rmaz, sadece kÃ¼tÃ¼phaneler grubunu barÄ±ndÄ±rÄ±r.

Ã¶rnekler:

- `Alpine` (Ã¶zelliÄŸi ufak olmasÄ±)
- `Ubuntu`
- `Busybox` (Ã¶zelliÄŸi ufak olmasÄ±)
- `CentOS`
- `Fedora`
- `Nanoserver` (`MS Windows` native `container`'lar iÃ§in)

## ğŸ“Œ container user

`container` Ã¼zerinde Ã§alÄ±ÅŸan uygulamalar `root` kullanÄ±cÄ± ile Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r. `Docker`, runtime sÄ±rasÄ±nda, `root` kullanÄ±cÄ±sÄ±nda komut yÃ¼rÃ¼tmemize izin verir.

## ğŸ“Œ stop vs remove komutlarÄ±

iki komutta container'Ä± durdurmaktadÄ±r. fakat remove komutu var olan state'i (container'Ä±n `file system`'Ä±nÄ±) silmektedir. oysa stop komutu state'i korumaktadÄ±r. bu ÅŸekilde state istenirse farklÄ± amaÃ§lar iÃ§in saklanabilir.

## ğŸ“Œ attach vs exec komutu

Docker iÃ§indeki root user'Ä±na komut satÄ±rÄ± Ã¼zerinden yÃ¶netimini saÄŸlar. attach olduÄŸumuzda yeni bir komut satÄ±rÄ± oturumu aÃ§Ä±lmaz, onun yerine son Ã§alÄ±ÅŸtÄ±rÄ±lan komutun Ã§Ä±ktÄ±larÄ±nÄ± gÃ¶rÃ¼rÃ¼z. eÄŸer yeni bir komut satÄ±rÄ± istiyor isek; "docker exec -i -t 878978547 bash command arg1 arg2" ile yeni bir bash aÃ§abiliriz. exec komutu en saÄŸda yazan komutu container Ã¼zerinde Ã§alÄ±ÅŸtÄ±rmamÄ±zÄ± saÄŸlar.

## ğŸ“Œ Docker build command

Dockerfile'Ä±n image olarak hazÄ±r hale getirilmesini saÄŸlar. run edip anÄ±nda ayaÄŸa kalkmasÄ± iÃ§in build olmasÄ± ÅŸart. Ã¶rneÄŸin; build sÄ±rasÄ±nda Docker iÃ§in gerekli dosyalarÄ± internetten indiriyor.

## ğŸ“Œ Docker run command

build edilmiÅŸ bir image'yi run etmemize yarar.

-d parametresi eklenince container'Ä±n ID'sini dÃ¶ndÃ¼rÃ¼p arka planda Ã§alÄ±ÅŸtÄ±rÄ±r. -d verilmezse komut satÄ±rÄ±nda OS'un system.out log'larÄ±nÄ± basar.

arka planda Ã§alÄ±ÅŸan container'Ä±n system.out Ã§Ä±ktÄ±larÄ±nÄ± "Docker log" komutu ile de gÃ¶rebiliriz.

## ğŸ“Œ Docker commit komutu

bu komut ile Ã§alÄ±ÅŸmakta olan bir image, kopyalanÄ±yor ve yeni bir image yaratÄ±lÄ±yor. bu image'nin ID'si ekrana basÄ±lÄ±yor. Ã¶rneÄŸin; MySQL image'si run edildi, daha sonra Ã¼zerine manuel olarak wget kuruldu. bu image kopyalanÄ±p saklanÄ±lmasÄ± isteniyor ise; commit komutundan yararlanÄ±lmalÄ±dÄ±r. aslÄ±nda yeni bir isimde klon image oluÅŸturulmuÅŸ oluyor.

"commit" yerine Dockerfile dosyasÄ±nÄ±n update edilmesi Ã¶neriliyor. Ã§Ã¼nkÃ¼ image'in nasÄ±l yaratÄ±ldÄ±ÄŸÄ±nÄ± daha ilerdeki zamanlarda net bilebiliriz.

## ğŸ“Œ layer

build komutu sÄ±rasÄ±nda container iÃ§inde yapÄ±lan her komut sonrasÄ± yeni bir katman yaratÄ±yor.

## ğŸ“Œ Docker diff command

"Docker diff containerId" komutu verildiÄŸinde o containerId'nin yaratÄ±ldÄ±ÄŸÄ± image-layer'Ä± ile ÅŸu andaki layer arasÄ±ndaki `file system`'daki farklarÄ± listeleyecektir.

## ğŸ“Œ kill vs stop command

stop container'larÄ±n iÃ§indeki tÃ¼m uygulamalara SIGTERM, kill komutu ise SIGKILL sinyalini yollar.

## ğŸ“Œ -p parameter

container'Ä± run ederken port numarasÄ± mapping'i yapmak iÃ§in kullanÄ±lÄ±r. Ã¶rneÄŸin;

> Docker run -p 80:8080 imageId

Bu ÅŸekilde host makinenin 80 portuna gelen istekler, Docker container iÃ§indeki 8080 portuna yÃ¶nlendirilecektir.

## ğŸ“Œ -v parameter

-p parametresi ile aynÄ± formata kullanÄ±lÄ±yor. fakat dizin baÄŸlamak (mount etmek) iÃ§in yapÄ±lÄ±yor. Ã¶rnek:

> docker run -v /home/host-user/dir:/home/Docker-user/dir imageId

## ğŸ“Œ docker inspect

inspect yanÄ±na yazÄ±lan ID, herhangi bir kaynaÄŸÄ±n ID'si olabilir. Ã¶rnek: volume ID, image ID, container ID, network ID, stack ID gibi.. tÃ¼m detaylÄ± JSON formatÄ±nda bize dÃ¶ndÃ¼rÃ¼r.

## ğŸ“Œ docker --link

Docker container'larÄ±n birer IP si vardÄ±r. bu IP'ler birbirine eriÅŸebilmelidirler. bu yÃ¼zden host dosyasÄ±na DNS kaydÄ± otomatik atÄ±labilir. Ã¶rnek:

> docker run --link myOtherContainerId:aliasOnDnsHostFile imageId

myOtherContainerId IP'si 10.0.0.1 ise; yeni yaratÄ±lan container iÃ§indeki host dosyasÄ±na bakÄ±ldÄ±ÄŸÄ±nda bu satÄ±r gÃ¶rÃ¼lecektir:

> 10.0.0.1 aliasOnDnsHostFile

## ğŸ“Œ Docker hub'a image yollama

Docker login komutu ile komut satÄ±rÄ±ndan Docker'a login olunabilir. daha sonra herhangi bir image'Ä± push komutu ile Docker hub'daki hesabÄ±mÄ±za yollayabilir.

## ğŸ“Œ repository vs registry

repository bir Docker-image'sinin farklÄ± sÃ¼rÃ¼mlerinin barÄ±ndÄ±rÄ±ldÄ±ÄŸÄ± sunucu yazÄ±lÄ±mÄ±dÄ±r. tag ise Docker-image'nin sÃ¼rÃ¼mÃ¼dÃ¼r. repository kelimesi birÃ§ok farklÄ± Docker-image'ini barÄ±ndÄ±rÄ±yormuÅŸ gibi gÃ¶rÃ¼nÃ¼yor, oysa aynÄ± Docker-image'inin farklÄ± sÃ¼rÃ¼mlerini barÄ±ndÄ±rÄ±yor. Ã¶rneÄŸin: repo:JDK tag:8.

registry ise birÃ§ok farklÄ± Docker-image'sinin bulunduÄŸu sunucu yazÄ±lÄ±mÄ±dÄ±r. Ã¶rnek "Docker hub", Docker Inc firmasÄ±nÄ±n resmi registry'sidir ve default olarak Docker client'ta tanÄ±mlÄ± gelir.

Yani 1 registry'de, birden fazla repository vardÄ±r. ama tersi doÄŸru deÄŸildir. Ã–rneÄŸin; Docker Hub'da (registry), Java repository'si vardÄ±r.

## ğŸ“Œ Docker ce

community edition (free)

## ğŸ“Œ Docker ee

enterprise edition

## ğŸ“Œ Docker edge vs stable

edge is beta version.

## ğŸ“Œ Docker Trusted Registry (âŸ· DTR)

locale kurulabilen registry.

## ğŸ“Œ Moby

2017 yÄ±lÄ± ile Docker aÃ§Ä±k kaynaklÄ± proje olarak Moby projesini baz almaya baÅŸladÄ±. Moby projesi DockerCE ve EE olarak Docker tarafÄ±ndan kapatÄ±lÄ±p Ã¼zerinde bazÄ± deÄŸiÅŸiklikler yapÄ±p daÄŸÄ±tÄ±lmaktadÄ±r.

## ğŸ“Œ union file system (âŸ· UnionFS)

`UnionFS`, `file system`'in Ã¶zel ismidir.

"__Union mount__" ise dosya sistemlerinde olan bir feature'dir. Bu Ã¶zellik desteklenen dosya sistemlerinde, bir dosyanÄ±n farklÄ± branch'leri (versiyonlarÄ±) olabilmektedir.

UnionFS'i, Docker, run edilen container'lar iÃ§in kullanÄ±r. Ã¶rnek: Java kurduk ve uygulamamÄ±zÄ± run ettik. Java ve Ubuntu-OS otomatik kurulur. bu 2 container read-only'dir. run ettiÄŸimiz Java uygulamasÄ± bir ÅŸeyler kaydetti. bu dosyalar Ubuntu-OS'un dosyalarÄ±nÄ± da deÄŸiÅŸtirmiÅŸ olabilir yada ek dosya atayabilir. bÃ¶yle durumlar iÃ§in `file system` bÃ¶yle iÅŸliyor: Ubuntu-OS ve Java'nÄ±n olduÄŸu bir `file system` read-only (no:1) oluÅŸturuluyor. daha sonra farklÄ± bir `file system`'a (no:2) Java uygulamamÄ±z dosya yazÄ±yor. eÄŸer container iÃ§inden bir dosya okumaya kalkarsak; UnionFS Ã¶nce no:2 `file system`'a bakÄ±yor. eÄŸer bir dosya silme bilgisi yada yeni dosya oluÅŸturulmuÅŸ ise bu dosyayÄ± bize dÃ¶nÃ¼yor. eÄŸer hiÃ§bir bilgi bulamaz ise no:1 `file system`'daki dosyayÄ± dÃ¶nÃ¼yor. bu sistemde no:2 `file system`'a no:1'in branch'i adÄ± veriliyor. istediÄŸimiz kadar branch oluÅŸturabiliyoruz.

var olan ve container aÃ§Ä±ldÄ±ktan sonra dÃ¼zenlenen bir dosya branch'e kopyalanÄ±r ve dÃ¼zenlenir. tabi eÄŸer taÅŸÄ±nan bu dosya Ã§ok bÃ¼yÃ¼kse ilk kopyalama iÅŸlemi zaman alacaktÄ±r.

UnionFS'e birÃ§ok alternatif var. bu alternatifleri Docker engine'den seÃ§ebiliyoruz. bazÄ± dosya sistemleri tÃ¼m dosyayÄ± deÄŸilde, dosyayÄ± belli bloklara bÃ¶lÃ¼yor ve sadece deÄŸiÅŸen kÄ±smÄ±n bloÄŸunu taÅŸÄ±yor. bu tarz farklÄ± yÃ¶ntemler mevcut.

## ğŸ“Œ volume

bir container'a host bilgisayarÄ±mÄ±zdaki bir dizini mount edebiliriz. buna alternatif olarak sanal bÃ¶lÃ¼mler Docker tarafÄ±ndan oluÅŸturulabilir. bunlar host makina tarafÄ±ndan okunamazlar. mount ve volume karÅŸÄ±laÅŸtÄ±rÄ±lacak olursa asÄ±l Ã¶nemli avantaj: volume'larÄ±n Docker tarafÄ±ndan mount edilmesi olacaktÄ±r. host bilgisayarda mount edeceÄŸimiz dizinin yetkileri, `file system`'Ä±n desteklenip desteklenmeyeceÄŸini gibi sorunlarla karÅŸÄ±laÅŸabiliriz. oysa volume'larda bÃ¶yle bir sorun yok.

## ğŸ“Œ Docker temizlik

### ğŸ“ŒğŸ“Œ Docker system prune

prune TÃ¼rkÃ§e kelime anlamÄ±: budamak.

bu komut sÄ±rasÄ± ile ÅŸu komutlarÄ± iÅŸletiyor:

- Docker container prune

- Docker image prune

- Docker network prune

- Docker volume prune

### ğŸ“ŒğŸ“Œ Docker system prune -a

-a parametresi durmuÅŸ tÃ¼m kaynaklarÄ±n silinmesini saÄŸlÄ±yor. yani o sÄ±rada run edilenler haricindeki tÃ¼m kaynaklar siliniyor. hiÃ§bir container run edilmezken Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa tÃ¼m kaynaklar silinir.

### ğŸ“ŒğŸ“Œ Docker container prune

stop olmuÅŸ tÃ¼m container'larÄ± siler.

### ğŸ“ŒğŸ“Œ Docker image prune

dangling image'leri siler.

### ğŸ“ŒğŸ“Œ Docker image prune -a

hiÃ§bir container tarafÄ±ndan kullanÄ±lmayan image'leri siler.

### ğŸ“ŒğŸ“Œ dangling volume

bir volume container run edildiÄŸinde mount edilir. yani container ile iliÅŸkilendirilir. eÄŸer iliÅŸkisiz bir volume var ise, bu volume dangling'dir. stop olmuÅŸ fakat silinmemiÅŸ container'lar hala volume'ler ile iliÅŸkilidir.

### ğŸ“ŒğŸ“Œ Docker volume prune

dangling volume'leri siler. bu komutun "-a" ile bir alternatifi yoktur. Ã§Ã¼nkÃ¼ zaten container ile baÄŸlantÄ±sÄ± olmayan her volume dangling'dir.

### ğŸ“ŒğŸ“Œ Docker network prune

volume ile aynÄ± mantÄ±ktadÄ±r. bu komutun "-a" ile bir alternatifi yoktur. Ã§Ã¼nkÃ¼ zaten container ile baÄŸlantÄ±sÄ± olmayan her volume dangling'dir.

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

## ğŸ“Œ VM disk formatlarÄ±

```text
 extension   native-support      support               aÃ§Ä±klama

- VMDK       VMWare              Virtualbox, QEMU/KVM

- VDI        VirtualBox          QEMU/KVM              Genel olarak VDI ve WDMK arasÄ±nda Ã¶nemli farklar mevcut deÄŸil.

- raw/img    QEMU/KVM                                  hiÃ§bir Ã¶zellik barÄ±ndÄ±rmÄ±yor. sanal diskte ne varsa direk burada binary olarak tutuluyor. format header dahi barÄ±ndÄ±rmÄ±yor. en iyi performans bunda.

- cow        QEMU/KVM

- qcow       QEMU/KVM            VirtualBox            cow'un yeni sÃ¼rÃ¼mÃ¼.

- qcow2      QEMU/KVM                                  qcow'un yeni sÃ¼rÃ¼mÃ¼.

- vhdx       Hyper-V             VirtualBox            vhd'nin yeni sÃ¼rÃ¼mÃ¼.

- vhd        Hyper-V             VirtualBox

- cloop      QEMU/KVM                                  Ã¶zel amaÃ§lÄ± kullanÄ±lan bir format.

- qed        QEMU/KVM            VirtualBox            qcow2'nin Ã¶zellikleri silinerek performans kazandÄ±rÄ±lmaya Ã§alÄ±ÅŸÄ±lÄ±yor. fakat proje durduruldu.
```

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢

## ğŸ“Œ OpenStack

aÃ§Ä±k kaynaklÄ± bulut biliÅŸim altyapÄ± projesidir. `AWS` gibi bulut biliÅŸim sunan bir firmada olmasÄ± gereken tÃ¼m yazÄ±lÄ±mlar bu proje altÄ±nda aÃ§Ä±k kaynaklÄ± olarak geliÅŸtirilmektedir.

projede Ã¶ncelik `IaaS`'tÄ±r.

`OpenStack` projesi resmi sitesinde `OS` diye geÃ§iyor, ama `OS` Ã¼zerinde Ã§alÄ±ÅŸan bir yazÄ±lÄ±mdÄ±r.

`OpenStack` projesi eklenti alyapÄ±sÄ±na sahiptir.

## ğŸ“Œ OpenShift

 | name                                                                  | description                                                  |
 |-----------------------------------------------------------------------|--------------------------------------------------------------|
 | `OKD (âŸ· Origin Community Distribution âŸ· old-name:OpenShift Origin)`   | aÃ§Ä±k kaynaklÄ± proje. community desteÄŸi var sadece.           |
 | `OpenShift Container Platform (âŸ· OCP)`                                | `OKD`'nin forku. enterprise destek sunuluyor.                |
 | `Red Hat OpenShift Online (âŸ· RHOO)`                                   | `Red Hat`'Ä±n kendi sunucularÄ±ndaki hazÄ±r `OKD`               |
 | `OpenShift Dedicated`                                                 | `AWS` gibi sunucularda kurulan `OKD`                         |
 | `OpenShift.io`                                                        | Bulut tabanlÄ± geliÅŸtirme ortamÄ±ydÄ±. geliÅŸtirmesi durduruldu. |
 | `OpenShift Kubernetes Engine (âŸ· old-name:OpenShift Container Engine)` | Saf `K8s` ve ek birkaÃ§ Ã¶zellik.                              |
 | `OpenShift Platform Plus`                                             | `OCP`'ye ek bazÄ± Ã¶zellikler daha sunuluyor.                  |

`OCP`, kendi iÃ§inde `K8s`'i wrap ederek kullanan onun Ã¼zerine ek Ã¶zellikler sunan bir yazÄ±lÄ±mdÄ±r:

- hazÄ±r `Web UI` (web console)
- hazÄ±r operatÃ¶rler bulundurur:
  - 3Ã¼ncÃ¼ parti (fakat sertifikalÄ±) `CI/CD` ve Monitoring hizmetleri iÃ§in gÃ¶mÃ¼lÃ¼ entegrasyon Ã¶zelliÄŸi.
  - logging operatÃ¶rÃ¼ ile `vector` uygulamasÄ± her pod'dan direk log'larÄ± toplar.
- integrated image registry
- resmi Ã¼cretli destek
- gÃ¼venlik yamalarÄ± ek olarak sunulmaktadÄ±r (en azÄ±ndan aÃ§Ä±klarÄ±n `K8s`'e nazaran daha hÄ±zlÄ± kapatÄ±lacaÄŸÄ±nÄ±n garantisini vermeye Ã§alÄ±ÅŸÄ±yorlar)

`OpenShift`, `oc` komut satÄ±rÄ± uygulamasÄ± yada web arayÃ¼zÃ¼nden yÃ¶netilmektedir.

`Openshift`'te her proje, `K8s`'teki namespace'e denk gelmektedir. DiÄŸer daha ufak birimler tamamiyle aynÄ± isimde kullanÄ±lÄ±r.

`OpenShift`'in yÃ¶netim paneli iÃ§in sunduÄŸu web arayÃ¼zÃ¼nde, bir proje iÃ§erisinde yeni bir `Pod` ayaÄŸa kaldÄ±rÄ±lmak istendiÄŸinde `namespace` bilgisi ister. burada istenen `namespace` bilgisi, `Pod`'un Ã§alÄ±ÅŸacaÄŸÄ± `namespace` deÄŸil, `OpenShift`'in sunduÄŸu `imagestream` Ã¶zelliÄŸi iÃ§in kullanÄ±lacak namespace bilgisidir.

### ğŸ“ŒğŸ“Œ Minishift

komut satÄ±rÄ± uygulamasÄ±dÄ±r. lokalde tek cluster `OpenShift` ayaÄŸa kaldÄ±rÄ±r.

## ğŸ“Œ Service Mesh

`Mesh` TÃ¼rkÃ§e kelime anlamÄ±: Ã§ark, kafes.

microservice'lerin aÄŸ yÃ¶netimi konularÄ± iÃ§in sunulan Ã§Ã¶zÃ¼mlere verilen genel bir terimdir.

Ele aldÄ±ÄŸÄ± ve Ã§Ã¶zdÃ¼ÄŸÃ¼ bazÄ± konular:

- `sidecar proxy`
- `service discovery`
- `load balancing`
- `circuit breaker`
- `SSL` (but not `Oauth`)
- monitoring network tools

Fakat bazÄ± `service mesh` framework'leri plugin'ler ile veya

`istio` sadece `service mesh` yÃ¶netimi sunar.

`OpenShift` hem `orchestration` hem de `service mesh` Ã¶zellikleri sunar.

## ğŸ“Œ Istio

`K8s` iÃ§erisinde oluÅŸturduÄŸumuz her `Pod` iÃ§inde, istio (otomatik olarak) kendi container'larÄ±nÄ± aÃ§ar. bu container'lar ile sidecar proxy pattern'ini baz alarak load balancing, logging gibi birÃ§ok servise mesh iÅŸlemimizi halleder.

'istioctl' komut satÄ±rÄ± uygulamasÄ± ile yÃ¶netimi saÄŸlar.

## ğŸ“Œ Kubernetes (âŸ· abb:K8s)

`UCP`, `Docker swarm` ve `Docker compose`'a alternatif olan ve daha fazla Ã¶zelliÄŸi iÃ§eren aÃ§Ä±k kaynaklÄ± yazÄ±lÄ±mdÄ±r. `K8s` container'larÄ± yÃ¶netmek iÃ§in kullanÄ±lÄ±r. container'larÄ±n ne ile yÃ¶netildiÄŸi ile ilgilenmez. yani `K8s`, `Docker` yada `Rkt` ile de Ã§alÄ±ÅŸabilir. kaldÄ±rdÄ±ÄŸÄ± container'larÄ± istediÄŸimiz herhangi bir container yazÄ±lÄ±mÄ± ile kaldÄ±rabiliriz.

### ğŸ“ŒğŸ“Œ command line tools

aÅŸaÄŸÄ±daki tÃ¼m komut satÄ±rÄ± uygulamalarÄ± `K8s`'ten ve birbirlerinden baÄŸÄ±msÄ±zdÄ±r. sadece istediÄŸimiz paketin binary'sini indirip lokalde Ã§alÄ±ÅŸtÄ±rabiliriz.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Kubectl

`K8s` command line tool'udur.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Minikube

komut satÄ±rÄ± uygulamasÄ±dÄ±r. lokalde test amaÃ§lÄ± Ã§alÄ±ÅŸmalar iÃ§in geliÅŸtirilmiÅŸtir. local makinede `VM` aÃ§makta ve iÃ§erisinde tek node'lu `K8s` yÃ¼rÃ¼tmektedir.

eÄŸer local makinede `Docker desktop` yÃ¼klÃ¼ ise; `VM`'siz de direk `K8s` yÃ¼rÃ¼tebilmektedir Ã§Ã¼nkÃ¼ `K8s` tool'larÄ± `Docker desktop` ile yÃ¼klÃ¼ gelmektedir.

`Minikube`, `Minikube` isminde bir `context` oluÅŸturuyor ve `Kubectl`'ye default `context` olarak set ediyor.

Minikube baÅŸlatma:

```sh
minikube start --driver="none"
```

Opsiyonel olarak vereceÄŸimiz driver'lar aÅŸaÄŸÄ±da listelenmiÅŸtir. Driver, cluster'Ä±n tÃ¼mÃ¼yle hangi ortama kurulacaÄŸÄ±nÄ± belirliyor.

- VirtualBox

VirtualBox Ã¼zerinde yeni VM aÃ§Ä±yor ve bunun iÃ§ine tÃ¼mÃ¼yle `K8s` admin ortamÄ±/cluster'Ä± kuruyor.

- Hyperv

VirtualBox ile aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±yor.

- KVM

VirtualBox ile aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±yor.

- VMware

VirtualBox ile aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±yor.

- none

`K8s` admin ortamÄ±nÄ± ve bunun Ã¼zerinde bir cluster'Ä± local OS'umuza kuruyor. Bu pek tavsiye edilen bir yÃ¶ntem deÄŸil. Ã‡Ã¼nkÃ¼ var olan OS'umuzdaki dosyalarda deÄŸiÅŸiklik yapÄ±yor.

`K8s` cluster'Ä± ve admin ortamÄ±nÄ± kuracaÄŸÄ±mÄ±z ortamda hem Docker requirement'i vardÄ±r hem de container'lar haricinde normal process'lerde Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve OS'un dosyalarÄ±na mÃ¼dahale gerÃ§ekleÅŸir. (zaten bunlardan kaÃ§Ä±nmak iÃ§in Minikube Ã§Ä±ktÄ± ve bu sebeple bu driver tavsiye edilmiyor)

- Docker

OS'umuzda bir Docker baÅŸlatÄ±yor. Bu Docker'Ä±n iÃ§erisine gerÃ§ek bir Docker kuruyor ve Kubeadm kurulumu yapÄ±yor.

kaynak: https://github.com/kubernetes/minikube/pull/6151 Pull request'in baÅŸlÄ±ÄŸÄ±nda "Kind" kullanÄ±ldÄ±ÄŸÄ± belirtilmiÅŸ.

__KIND__ `K8s`'in Docker ortamÄ±na kurulmasÄ± iÃ§in geliÅŸtirilen bir Docker-image projedir. KIND, Docker iÃ§erisine Docker kurulumu yapar ve image iÃ§erisine `K8s` kurar. kaynak: https://kind.sigs.k8s.io/docs/user/resources/ "Deep Dive: KIND - Benjamin Elder & Antonio Ojea" videosunda, 2:08'de slide ekranÄ±nda Docker iÃ§erisine neler kurulduÄŸu yazÄ±lmÄ±ÅŸ.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Kubeadm

komut satÄ±rÄ± uygulamasÄ±dÄ±r. `K8s`'in kendisini update edebilmemizi, cluster oluÅŸturabilmemizi saÄŸlÄ±yor.

### ğŸ“ŒğŸ“Œ components

### ğŸ“ŒğŸ“Œ Control Plane Components (âŸ· Master Components)

Cluster'daki tÃ¼m node'larÄ± ve sistemi yÃ¶neten modÃ¼llerin tÃ¼mÃ¼dÃ¼r. AÅŸaÄŸÄ±daki parÃ§alardan oluÅŸur:

#### ğŸ“ŒğŸ“ŒğŸ“Œ Kube-apiserver

API'nin saÄŸlanmasÄ± iÃ§in gereken module. API olarak servis sunuyor. buraya HTTP isteÄŸi olarak yollanan emirleri Kubelet'lere yolluyor. Kube-apiserver master node iÃ§erisindedir.

#### ğŸ“ŒğŸ“ŒğŸ“Œ etcd

key-value DB'dir. bÃ¼tÃ¼n sistemin meta bilgilerinin yÃ¶netimi buradan yapÄ±lÄ±r.

#### ğŸ“ŒğŸ“ŒğŸ“Œ kube-scheduler

scheduler'larÄ± dÃ¼zenli olarak Ã§alÄ±ÅŸtÄ±ran modÃ¼ldÃ¼r. aynÄ± zamanda yeni oluÅŸturulan `POD`'larÄ± node'lara atayan modÃ¼ldÃ¼r.

#### ğŸ“ŒğŸ“ŒğŸ“Œ kube-controller-manager

- node'larÄ±n ayakta olup olmadÄ±ÄŸÄ±nÄ± kontrol eder
- aÃ§Ä±lan `Pod`'larÄ±n doÄŸru adet olduÄŸunu kontrol eder

gibi iÅŸlevleri vardÄ±r.

#### ğŸ“ŒğŸ“ŒğŸ“Œ cloud-controller-manager

`kube-controller-manager`'in bulut hizmetler (`Amazon` `AWS` gibi) iÃ§in geliÅŸtirilmiÅŸ tÃ¼revidir.

### ğŸ“ŒğŸ“Œ node components

node bir VM yada fiziksel makinedir. node components, her node iÃ§inde yÃ¼klÃ¼ olan modÃ¼llerdir.

#### ğŸ“ŒğŸ“ŒğŸ“Œ Kubelet

her node'da bir adet yÃ¼klÃ¼dÃ¼r. bu yazÄ±lÄ±m kube-controller-manager'dan emirleri alÄ±r ve yerine getirir. o node Ã¼zerinde yeni Pod'larÄ±n ayaÄŸa kaldÄ±rÄ±lmasÄ± kapatÄ±lmasÄ± gibi iÅŸlevleri yerine getiriyor.

#### ğŸ“ŒğŸ“ŒğŸ“Œ kube-proxy

her node'da bir adet yÃ¼klÃ¼dÃ¼r. `K8s` service, port yÃ¶nlendirme, gibi network'sel iÅŸlemlerinin altyapÄ±sÄ±nÄ±n her node'da Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayan yazÄ±lÄ±mdÄ±r. network proxy yazÄ±lÄ±mÄ±dÄ±r.

### ğŸ“ŒğŸ“Œ Container Runtime

her node'da kurulmuÅŸ olmasÄ± gereken Docker yada alternatifi container manager'Ä±.

### ğŸ“ŒğŸ“Œ addons vs extension vs plugin

`K8s` dÃ¼nyasÄ±nda 3'Ã¼ birbirinden farklÄ± kavramlardÄ±r.

Plugin POD yÃ¶netimi yaparken kullanÄ±lan kaynaklar Ã¼zerinde manipÃ¼lasyon yapar. Ã¶rnek: "volume plugin".

Addon'lar direk `K8s`'in kendi Ã¶zelliklerini deÄŸiÅŸtirir. Yani `K8s` admin iÃ§in daha Ã§ok gerekli Ã¶zelliklerdir.

Extension'lar ise, `K8s` API'yi geniÅŸletir.

### ğŸ“ŒğŸ“Œ namespace, context, cluster

#### ğŸ“ŒğŸ“ŒğŸ“Œ namespace

her cluster'da birÃ§ok namespace olabilir. her namespace kendi iÃ§inde Pod ve servisler bulundurur.

tÃ¼m namespace'leri listele:
> kubectl get namespaces

yeni namespace yarat:
> kubectl create namespace dev

#### ğŸ“ŒğŸ“ŒğŸ“Œ context

kubectl komutunu Ã§alÄ±ÅŸtÄ±rÄ±rken, server'a login olacak bilgileri ve hangi namespace'te komut Ã§alÄ±ÅŸtÄ±racaÄŸÄ±mÄ±z bilgisi gereklidir. bu login ve namespace bilgilerinin bÃ¼tÃ¼nÃ¼ne birlikte "context" denir. Ã¶rneÄŸin bir context yaratalÄ±m:

> kubectl config set-context minidev --cluster=cluster1 --user=user1 --namespace=namespace1

switch to context:

> kubectl config use-context minidev

#### ğŸ“ŒğŸ“ŒğŸ“Œ cluster

`K8s` cluster altyapÄ±sÄ±nÄ± destekleyen platformdur. Ã¶rneÄŸin Google cloud, Amazon cloud bu yapÄ±yÄ± destekliyor. lokalden hangi cluster'a gideceÄŸimizi komut satÄ±rÄ±ndan belirtmemiz gerekmektedir.

Bir yazÄ±lÄ±m projesinin development, test, prod gibi ortamlarÄ±nÄ±n ayrÄ± cluster'lara (namespace'lere deÄŸil!) kurulmasÄ± Ã¶nerilir.

## ğŸ“Œ K8s Objects

`K8s`'te yaÅŸayan ve API Ã¼zerinden manage edilen her feature birer objedir. Obje listesi aÅŸaÄŸÄ±da verilmiÅŸtir.

- Pod
- service
- volume
- namespace
- Controllers
  - ReplicaSet
  - Deployment
  - StatefulSet
  - DaemonSet
  - Job

```yaml
apiVersion: v1
kind: Service # this is "object".
metadata:
  name: my-object-name
  labels:
    run: my-object-label 
spec:
  # the field under "spec", depend on the object type.
```

## ğŸ“Œ Pod

Pod tanÄ±mÄ±nÄ±n bulunduÄŸu dosya Ã¶rneÄŸi:

```yaml
apiVersion: v1 # in which K8s version this file works.
kind: Pod # can be other types like: Service, Pod...
metadata:
  name: multi-container-example # name of this Pod.
  namespace: test
  labels:
    app: web # labels are custom key-values. (labels are explained in another topic)
spec:
  containers:
  - name: nginx
    image: nginx:stable-alpine
    resources:
      limits:
        memory: "600Mi" # if it will try to use more memory, this Pod will be terminate automatically by K8s and will re-create again.
        CPU: "500m" # pod will not be killed if will try to use more then 500m CPU. source: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ title: "How Kubernetes applies resource requests and limits", paragraph: 2nd paragraph from the end.

        # CPU unit is the core CPU (virtual) count of the current machine. for example:
        # - 3000m ---> 3    CPU core of the current machine
        # - 10m   ---> 0.01 CPU core of the current machine
        
    ports:
    - containerPort: 80 # port which is exported to outside
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html # path which will mount inside container file system
  - name: content
    image: alpine:latest
    volumeMounts:
    - name: html
      mountPath: /html # path which will mount inside container file system
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          echo $(date)"<br>" >> /html/index.html;
          sleep 5;
        done
  volumes:
  - name: html # which is the reference name of this volume.
    emptyDir: {} # creates an empty volume.

    # alternative of "emptyDir":
    # hostPath:*
    #   path: /data # the full path of the namespace cluster machine
```

context'imizi komut satÄ±rÄ±ndan belirledikten sonra `Pod`'umuzu yaratabiliriz:

> kubectl create -f multi-container-example.yaml

YukarÄ±da 1 Pod iÃ§inde 2 adet container bulundurulmuÅŸtur. Mount ÅŸekli Docker'dan biraz farklÄ±dÄ±r. Mount tanÄ±mÄ± container tanÄ±mÄ±nÄ±n dÄ±ÅŸÄ±ndadÄ±r (Ã¼st seviyesindedir). bu mounting'e ihtiyacÄ± olan container'lar "mountPath" ile bu tanÄ±mÄ± kendileri iÃ§in kullanabilirler. YukarÄ±daki "html" isimli volume her 2 container tarafÄ±ndan kullanÄ±lmÄ±ÅŸ. bÃ¶yle durumlarda aynÄ± dosyalar 2 container tarafÄ±ndan da gÃ¶rÃ¼lÃ¼p yazÄ±labilmektedir.

AÅŸaÄŸÄ±daki komut ile var olan Pod'larÄ± listeleyebiliriz:

> kubectl get pods --show-labels

Log'a bakmak iÃ§in:

> kubectl logs -f pod-name --tail 200

EÄŸer bir Pod iÃ§erisinde birden fazla container varsa, sadece ilgili container'Ä±n log'una bakmak istiyorsak:

> kubectl logs -f pod-name --tail 200 -c container-name

get detail of running Pod:

> kubectl describe pod pod-name

delete Pod:

> kubectl delete pod pod-name

yada

> kubectl delete -f pod-manifest.yaml

## ğŸ“Œ port-forward

`pod-manifest.yaml` dosyamÄ±zda dÄ±ÅŸarÄ±ya aÃ§Ä±lacak portu belirtmezsek; sonradan komut satÄ±rÄ±nda ÅŸu ÅŸekilde portu dÄ±ÅŸarÄ±ya aÃ§abiliriz:

> kubectl port-forward pod-name 8081:8080

## ğŸ“Œ CNI (âŸ· Container network interface)

`K8s` network kurmaz, port yÃ¶nlendirmeleri yapmaz. AracÄ± bir plugin kullanÄ±r. O plugin2lere ne yapmalarÄ± gerektiÄŸini sÃ¶yler.

Ã¶rnek plugin'ler: `Calico`, `Flannel`, `Weave Net`.

`K8s`'te default olarak CNI yÃ¼klÃ¼ gelmez. Bu sebeple eklentisiz overlay network yetenekleri Ã§alÄ±ÅŸmaz.

## ğŸ“Œ label and selector

`Pod` veya `K8s` service `YAML`'mizin baÅŸÄ±nda istediÄŸimiz isimde label'lar kullanabiliriz. bu label'lar sayesinde daha sonra birÃ§ok sorgu, update gibi iÅŸlemleri belli `Pod` veya servislerimize saÄŸlamÄ±ÅŸ olacaÄŸÄ±z.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-example
  labels:
    app: nginx
    environment: prod
    # others labels here
```

## ğŸ“Œ service configuration

`K8s` service; selector'ler ile belirttiÄŸimiz `Pod` grubuna referans edecek olan sanal yapÄ±dÄ±r. Ã¶rneÄŸin; bir `K8s` service istek yaparsak, aslÄ±nda ona baÄŸlÄ± `Pod`'lardan bir tanesine yapmÄ±ÅŸ oluruz.

example `YAML` file:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service
  labels:
    run: user-service # the label of this service.
spec:
  type: NodePort # there are other alternatives for this.
  ports:
    - port: 8080 # port of this service (only for communication between POD's. not outside of the cluster.)
                 # If it is null (as default) than  port = targetPort.

      targetPort: 8081 # when run a dockerfile, it expose a port. this port is the exposed port.
                       # example redis works 7777 as default. on dockerfile we expose that port via 8888.
                       # in this case 8888 must be the targetPort.

      nodePort: 8083 # the exported port of this service for outside of the cluster.
                     # on K8s we must have at least 1 cluster.
      
      protocol: TCP
      name: http-port # this is only a label for this port.
    - port: 8090
      targetPort: 8091
      nodePort: 8093
      protocol: TCP
      name: metrics-port
  selector:
    run: user-service-pod-label # this service is in front of all the POD's which have "user-service-pod-label".
```

## ğŸ“Œ alternatives of "type"

- __NodePort__
  
  clsuter dÄ±ÅŸÄ±na port aÃ§abilmemizi saÄŸlayan service tipi.

- __ClusterIP__ (default)

  cluster dÄ±ÅŸÄ±ndan eriÅŸilemeyen tamamen sanal private bir `IP` yaratÄ±p bunu bir container'a atayabilmemizi saÄŸlar.
  
  bu internal programlar iÃ§in tercih edilir. Ã–rneÄŸin DB. dÄ±ÅŸarÄ±dan eriÅŸim olsun istenmez.

- __LoadBalancer__

  Cluster'daki implementasyona gÃ¶re Ã§alÄ±ÅŸan bir yapÄ±dÄ±r. Ã–rneÄŸin; cloud sistemlerde direk native load balancer'Ä± kullanÄ±r.

  `K8s` default olarak bir implementasyon sunmaz. dolaysÄ± ile dÄ±ÅŸarÄ±ya port aÃ§amayÄ±z.

  `minikube` de bir implementasyon sunmaz. fakat `minikube` run olurken, ayrÄ± bir terminal'de ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±rsak:

  `minikube tunnel`

  yeni aÃ§Ä±lan process, `K8s` API'sine baÄŸlanÄ±r ve kendini bir load balancer gibi tanÄ±tÄ±r. bÃ¶ylece dÄ±ÅŸarÄ±ya `IP` aÃ§abiliriz.

- __ExternalName__

  `DNS` kaydÄ± oluÅŸturabiÅŸmemizi saÄŸlar. bÃ¶ylece dÄ±ÅŸ dÃ¼nyaya veya iÃ§erdeki bir `pod`'a direk bu `DNS` Ã¼zerinden istek atabiliriz.

## ğŸ“Œ Ingress

`K8s` service type deÄŸildir. kendisi baÅŸlÄ±ca `K8s` service yapÄ±sÄ±na alternatif bir Ã¶zelliktir.

`NodePort` yapÄ±sÄ±na benzerdir. fakat `Ingress` ek olarak; gelen istekteki URL-PATH deÄŸerine gÃ¶re istediÄŸimiz servise yÃ¶nlendirme yapabileceÄŸimiz kurallar dizisi tanÄ±mlayabilmekteyiz.

## ğŸ“Œ deployment

`K8s` deployment iÃ§erisine birÃ§ok `replicaSet` ve `Pod` barÄ±ndÄ±rabilen bir yapÄ±dÄ±r.

`YAML` iÃ§erisinde "`kind: deployment`" yazmamÄ±z gereklidir.

## ğŸ“Œ Replica Controller

istediÄŸimiz `Pod` sayÄ±sÄ±nÄ±n ayakta kalmasÄ±nÄ± saÄŸlar.

`YAML` iÃ§erisinde "`kind: ReplicationController`" yazmamÄ±z gereklidir.

## ğŸ“Œ Replica Set

`Replica Controller`'Ä±n alternatifidir. temelde aynÄ± gÃ¶revi gÃ¶rÃ¼r.

`YAML` iÃ§erisinde "`kind: ReplicaSet`" yazmamÄ±z gereklidir.

## ğŸ“Œ desired state vs actual state

her `K8s` objesinin bir 2 state'i vardÄ±r. biri yazÄ±lÄ±mcÄ±nÄ±n istediÄŸi durum, diÄŸer ise ÅŸu andaki durumudur. Ã¶rneÄŸin  `ReplicaSet` ile 100 adet container ayaÄŸa kaldÄ±rmak istemiÅŸ olalÄ±m, fakat henÃ¼z sadece 50 tanesi ayakta olsun. bu durumda: 

- `desired state` 100
- `actual state`":50

demektir.

## ğŸ“Œ health check

`Health check` iÅŸlemi baÅŸarÄ±sÄ±z olursa `K8s` 2 farklÄ± aksiyon alabilir:

- __Readiness probe__

  `probe` kelime anlamÄ±: soruÅŸturma/yoklama.

  EÄŸer `probe` iÅŸlemi (yapÄ±lan bu `health check` iÅŸlemi) baÅŸarÄ±sÄ±z olursa, `K8s` bu `pod`'a gelen request'leri yÃ¶nlendirmez.

- __Liveness probe__

  EÄŸer `probe` iÅŸlemi (yapÄ±lan bu `health check` iÅŸlemi) baÅŸarÄ±sÄ±z olursa, `K8s` bu `pod`'u kill eder.

`K8s` `Health check` yapabilmek iÃ§in 3 farklÄ± seÃ§enek sunuyor:

- `HTTP` isteÄŸi (`pod`'un `health check` iÃ§in belirlenen HTTP isteÄŸine 200 dÃ¶nmesi yeterli)
- `TCP` isteÄŸi (`K8s` `health check` iÃ§in belirlenen `TCP` portuna baÄŸlanma isteÄŸi yapar. EÄŸer connection aÃ§Ä±lÄ±rsa, `pod` `health check`'i baÅŸarÄ±lÄ±dÄ±r anlamÄ±na gelir)
- Komut (`K8s` ilgili `pod`'da bir komut Ã§alÄ±ÅŸtÄ±rÄ±r. eÄŸer komut, exit code 0 dÃ¶ndÃ¼rÃ¼rse o zaman `pod` saÄŸlÄ±klÄ± anlamÄ±na gelir)

## ğŸ“Œ distrobox vs toolbx

birbirlerine alternatif projelerdir.

Ã§ok kolay ÅŸekilde `$HOME` ve `GUI` nin `host` ile ortak olarak kullanÄ±lmasÄ± iÃ§in gerekli config'leri container'Ä± initialize ederken ona parametre geÃ§iyor.

`distrobox` %100 `shell` script projesidir. resmi sitesindeki `install` komutu, sadece `shell` script'leri bi dizine atar. yani tamamen portable'dÄ±r.

`toolbx` ise `golang` ile yazÄ±lmÄ±ÅŸ bir projedir.

`distrobox` config dosyasÄ± tutmaz. onun yerine her aÃ§tÄ±ÄŸÄ± container'a `label` atar. oradaki `label`'lardan hangi container'Ä±n `distrobox` tarafÄ±ndan manage edilip edilmediÄŸini anlar.

Ã¶rnek komutlar:

```sh
distrobox create -n ubuntu1 -i ubuntu:24.10

###### installs basic apps
###### and log-in to shell of guest OS.
distrobox enter ubuntu1

###### note:
###### - you can check the installing apps and mounted points from 
###### /distrobox-installation-path/bin/distrobox-init.sh file.
###### - "flatpak" executable is linked from host, if it is installed on host.
```

`toolbx`, `fedora`'da yÃ¼klÃ¼ geliyor.

```sh
toolbox create fedora1 --image registry.fedoraproject.org/fedora-toolbox:41

toolbox enter fedora1
```

Bu projede: https://github.com/toolbx-images/images `enter` yapmadan Ã¶nce gerekli konfigurasyonlarÄ±n ve paketlerin yÃ¼klÃ¼ olduÄŸu image'ler yaratÄ±lÄ±yor. opsiyonel olarak bu image'ler `distrobox` veya `toolbx` tarafÄ±ndan kullanÄ±labilir. Fakat bunlar dÄ±ÅŸÄ±nda istediÄŸimiz image'yi indirip, `distrobox` veya `toolbx` ile kullanabiliriz. hazÄ±r image'lerde hangi paketlerin yÃ¼klÃ¼ olduÄŸunu burada detaylarÄ±nÄ± Ã¶grenebiliriz:

https://raw.githubusercontent.com/toolbx-images/images/refs/heads/main/centos/stream10/Containerfile

## ğŸ“Œ Helm

`Helm` kelime anlamÄ±: dÃ¼men.

`Helm` komut satÄ±rÄ± uygulamasÄ±dÄ±r. `K8s` client'Ä±nÄ± wrap eder (arkada kubectl'i call ederek Ã§alÄ±ÅŸÄ±r) ve sadece `Helm` tanÄ±mlamalarÄ± ile daha kolay `K8s` ortamÄ±na deploy yapabilmemizi saÄŸlar.

Server-side tarafta `Helm`'e ait Ã¶zel bir kurulum yapÄ±lmasÄ± gerekmemektedir.

## ğŸ“Œ Bitnami

`Bitnami` projesi altÄ±nda bulut sistemler, `Docker`, `Helm` gibi sistemler iÃ§in hazÄ±r `VM` veya container'lar sunulmaktadÄ±r. BunlarÄ± sunarken de her platform iÃ§in ortak ayarlar sunulmaktadÄ±r. Bu ÅŸekilde bir kere yapÄ±lan ayarlar tÃ¼m ortamlarda kolay taÅŸÄ±nabilirlik saÄŸlamaktadÄ±r. `kaynak: https://hub.docker.com/r/bitnami/mariadb "Bitnami containers, virtual machines and cloud images use the same components and configuration approach - making it easy to switch between formats based on your project needs.".`

## ğŸ“Œ Kustomize

Burada Ã§ok basit bir anlatÄ±m mevcut: https://github.com/kubernetes-sigs/kustomize/tree/master/examples/helloWorld

AÅŸaÄŸÄ±daki gibi bir dosya yapÄ±sÄ± yaratÄ±yoruz. production ve `uat-1` gibi farklÄ± ortamlarÄ±mÄ±z olsun. Sadece farklÄ±lÄ±klarÄ± `overlays` dizini altÄ±na koyuyoruz.

```text
/project-directory
â”œâ”€â”€ base
â”‚   â”œâ”€â”€ configMap.yaml --> K8s'in native tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
â”‚   â”œâ”€â”€ deployment.yaml --> K8s'in native tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
â”‚   â”œâ”€â”€ kustomization.yaml --> Kustomize'Ä±n tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
â”‚   â””â”€â”€ service.yaml --> K8s'in native tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
â””â”€â”€ overlays
    â”œâ”€â”€ production
    â”‚   â”œâ”€â”€ deployment.yaml --> K8s'in native tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
    â”‚   â””â”€â”€ kustomization.yaml --> Kustomize'Ä±n tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
    â””â”€â”€ uat-1
        â”œâ”€â”€ kustomization.yaml --> Kustomize'Ä±n tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
        â””â”€â”€ map.yaml --> K8s'in native tanÄ±dÄ±ÄŸÄ± dosya biÃ§imi
```

ArtÄ±k direk komut satÄ±rÄ±ndan ilgili ortama deploy alabiliriz:

```sh
OVERLAYS=/project-directory/overlays/production
kustomize build $OVERLAYS/production
kustomize build $OVERLAYS/production | kubectl apply -f -
```

## ğŸ“Œ Minikube vs K3s vs KIND (âŸ· Kubernetes IN Docker)

`KIND`, `K8s`'i `Docker` iÃ§erisinde Ã§alÄ±ÅŸtÄ±ran bir projedir. `minikube`, `VM`'de deÄŸilde, `Docker` driver'Ä± ile initialize edilirse, `KIND` projesinden yararlanÄ±yor.

`Minikube`, saf `K8s`'i olduÄŸu gibi kullanÄ±yor. sadece, initializer `Kubectl` ve `Kubeadm`'yi wrap ediyor ve kurulumlarÄ± spesifik config'ler ile gerÃ§ekleÅŸtiriyor.

`K3s`, resmi `Github` sayfasÄ±ndaki `Readme` dosyasÄ±nda yazana gÃ¶re: https://github.com/k3s-io/k3s saf `K8s`'i upstream olarak kullanÄ±yorlar fakat patch uygulayarak ÅŸu gibi farklÄ±lÄ±klar uygulamÄ±ÅŸlar:

- bazÄ± Ã¶zellikleri silinmiÅŸ:
  - In-tree storage drivers
  - In-tree cloud provider
- ortalama 1000 satÄ±rlÄ±k code deÄŸiÅŸikliÄŸi var
- var olan Ã¶zellikler deÄŸiÅŸtiriliyor:
  - `Etcd3` yerine `Sqlite3` kullanÄ±lmÄ±ÅŸ.
- ek Ã¶zellikler iÃ§eriyor:
  - Managing the `SSL` certificates of `K8s` components
  - Managing the connection between worker and server nodes
  - Auto-deploying `K8s` resources from local manifests in realtime as they are changed.
